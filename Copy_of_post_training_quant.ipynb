{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedFarrukh/DeepLearning-EdgeComputing/blob/main/Copy_of_post_training_quant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-GR0EDHM1SO"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "R3yYtBPkM2qZ"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y8E0lw5eYWm"
      },
      "source": [
        "# Post-training dynamic range quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIGrZZPTZVeO"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/performance/post_training_quant\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/tensorflow/tensorflow/lite/g3doc/performance/post_training_quant.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://tfhub.dev/google/imagenet/resnet_v2_101/classification/4\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub model</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTC1rDAuei_1"
      },
      "source": [
        "## Overview\n",
        "\n",
        "[TensorFlow Lite](https://www.tensorflow.org/lite/) now supports\n",
        "converting weights to 8 bit precision as part of model conversion from\n",
        "tensorflow graphdefs to TensorFlow Lite's flat buffer format. Dynamic range quantization achieves a 4x reduction in the model size. In addition, TFLite supports on the fly quantization and dequantization of activations to allow for:\n",
        "\n",
        "1.  Using quantized kernels for faster implementation when available.\n",
        "2.  Mixing of floating-point kernels with quantized kernels for different parts\n",
        "    of the graph.\n",
        "\n",
        "The activations are always stored in floating point. For ops that\n",
        "support quantized kernels, the activations are quantized to 8 bits of precision\n",
        "dynamically prior to processing and are de-quantized to float precision after\n",
        "processing. Depending on the model being converted, this can give a speedup over\n",
        "pure floating point computation.\n",
        "\n",
        "In contrast to\n",
        "[quantization aware training](https://github.com/tensorflow/tensorflow/tree/r1.14/tensorflow/contrib/quantize)\n",
        ", the weights are quantized post training and the activations are quantized dynamically\n",
        "at inference in this method.\n",
        "Therefore, the model weights are not retrained to compensate for quantization\n",
        "induced errors. It is important to check the accuracy of the quantized model to\n",
        "ensure that the degradation is acceptable.\n",
        "\n",
        "This tutorial trains an MNIST model from scratch, checks its accuracy in\n",
        "TensorFlow, and then converts the model into a Tensorflow Lite flatbuffer\n",
        "with dynamic range quantization. Finally, it checks the\n",
        "accuracy of the converted model and compare it to the original float model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XsEP17Zelz9"
      },
      "source": [
        "## Build an MNIST model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDqqUIZjZjac"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyqAw1M9lyab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a2a3e5a-5538-4bcf-d29c-e51106a12b74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pathlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ6Q0qqKZogR"
      },
      "source": [
        "### Train a TensorFlow model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWSAjQWagIHl",
        "outputId": "2fed0767-a9a7-443d-d2d5-a11a00bd6493",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "1875/1875 [==============================] - 49s 25ms/step - loss: 0.2824 - accuracy: 0.9194 - val_loss: 0.1421 - val_accuracy: 0.9590\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78e9cc932e00>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 to 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation=tf.nn.relu),\n",
        "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=1,\n",
        "  validation_data=(test_images, test_labels)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NMaNZQCkW9X"
      },
      "source": [
        "For the example, since you trained the model for just a single epoch, so it only trains to ~96% accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl8_fzVAZwOh"
      },
      "source": [
        "### Convert to a TensorFlow Lite model\n",
        "\n",
        "Using the TensorFlow Lite [Converter](https://www.tensorflow.org/lite/models/convert), you can now convert the trained model into a TensorFlow Lite model.\n",
        "\n",
        "Now load the model using the `TFLiteConverter`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i8B2nDZmAgQ",
        "outputId": "16504dbe-9d1a-43ea-fb7a-c65664d8742e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpdx8mgcuz/assets\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2o2ZfF0aiCx"
      },
      "source": [
        "Write it out to a tflite file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vptWZq2xnclo"
      },
      "outputs": [],
      "source": [
        "tflite_models_dir = pathlib.Path(\"/tmp/mnist_tflite_models/\")\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie9pQaQrn5ue",
        "outputId": "860635da-1008-4348-892a-987b8307fd8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84820"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BONhYtYocQY"
      },
      "source": [
        "To quantize the model on export, set the `optimizations` flag to optimize for size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8PUvLWDlmmz",
        "outputId": "e19f17a8-fb51-4665-84ff-b87f2f243950",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpod5ozesq/assets\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24064"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant_model = converter.convert()\n",
        "tflite_model_quant_file = tflite_models_dir/\"mnist_model_quant.tflite\"\n",
        "tflite_model_quant_file.write_bytes(tflite_quant_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhMmUTl4sbkz"
      },
      "source": [
        "Note how the resulting file, is approximately `1/4` the size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JExfcfLDscu4",
        "outputId": "ee208852-fb7f-4d05-c6b5-299be56edf93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 108K\n",
            "-rw-r--r-- 1 root root 24K Jun 16 20:23 mnist_model_quant.tflite\n",
            "-rw-r--r-- 1 root root 83K Jun 16 20:23 mnist_model.tflite\n"
          ]
        }
      ],
      "source": [
        "!ls -lh {tflite_models_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8lQHMp_asCq"
      },
      "source": [
        "## Run the TFLite models\n",
        "\n",
        "Run the TensorFlow Lite model using the Python TensorFlow Lite\n",
        "Interpreter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap_jE7QRvhPf"
      },
      "source": [
        "### Load the model into an interpreter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn16Rc23zTss"
      },
      "outputs": [],
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
        "interpreter.allocate_tensors()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8Pztk1mvNVL"
      },
      "outputs": [],
      "source": [
        "interpreter_quant = tf.lite.Interpreter(model_path=str(tflite_model_quant_file))\n",
        "interpreter_quant.allocate_tensors()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2opUt_JTdyEu"
      },
      "source": [
        "### Test the model on one image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKslvo2kwWac"
      },
      "outputs": [],
      "source": [
        "test_image = np.expand_dims(test_images[0], axis=0).astype(np.float32)\n",
        "\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "interpreter.set_tensor(input_index, test_image)\n",
        "interpreter.invoke()\n",
        "predictions = interpreter.get_tensor(output_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZClM2vo3_bm",
        "outputId": "08108d44-4f4c-435d-cca6-869845c38494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pylab as plt\n",
        "\n",
        "plt.imshow(test_images[0])\n",
        "template = \"True:{true}, predicted:{predict}\"\n",
        "_ = plt.title(template.format(true= str(test_labels[0]),\n",
        "                              predict=str(np.argmax(predictions[0]))))\n",
        "plt.grid(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwN7uIdCd8Gw"
      },
      "source": [
        "### Evaluate the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05aeAuWjvjPx"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "# A helper function to evaluate the TF Lite model using \"test\" dataset.\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  inf_time = 0\n",
        "  for test_image in test_images:\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    inf_time +=  time.time() - start_time\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  accurate_count = 0\n",
        "  for index in range(len(prediction_digits)):\n",
        "    if prediction_digits[index] == test_labels[index]:\n",
        "      accurate_count += 1\n",
        "  accuracy = accurate_count * 1.0 / len(prediction_digits)\n",
        "  avg_inf_time = inf_time/len(prediction_digits)\n",
        "\n",
        "  return (accuracy, avg_inf_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqXBnDfJ7qxL",
        "outputId": "c7f5960b-c784-4451-a6cf-5564030a30b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.959, 0.00011693315505981445)\n"
          ]
        }
      ],
      "source": [
        "print(evaluate_model(interpreter))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km3cY9ry8ZlG"
      },
      "source": [
        "Repeat the evaluation on the dynamic range quantized model to obtain:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9cnwiPp6EGm",
        "outputId": "bba97bff-b8f4-44da-e74d-a55338533ef7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.9587, 0.00011655685901641846)\n"
          ]
        }
      ],
      "source": [
        "print(evaluate_model(interpreter_quant))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7lfxkor8pgv"
      },
      "source": [
        "In this example, the compressed model has no difference in the accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0o1FtmWeKZm"
      },
      "source": [
        "## Optimizing an existing model\n",
        "\n",
        "Resnets with pre-activation layers (Resnet-v2) are widely used for vision applications.\n",
        "  Pre-trained frozen graph for resnet-v2-101 is available on\n",
        "  [Tensorflow Hub](https://tfhub.dev/google/imagenet/resnet_v2_101/classification/4).\n",
        "\n",
        "You can convert the frozen graph to a TensorFLow Lite flatbuffer with quantization by:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrXZxSJiJfYN"
      },
      "outputs": [],
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "resnet_v2_101 = tf.keras.Sequential([\n",
        "  keras.layers.InputLayer(input_shape=(224, 224, 3)),\n",
        "  hub.KerasLayer(\"https://tfhub.dev/google/imagenet/resnet_v2_101/classification/4\")\n",
        "])\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(resnet_v2_101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwnV4KxwVEoG",
        "outputId": "824b725f-1dfc-4ec3-d78b-2266d99dd19c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpbzmyrtcf/assets\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "178422328"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Convert to TF Lite without quantization\n",
        "resnet_tflite_file = tflite_models_dir/\"resnet_v2_101.tflite\"\n",
        "resnet_tflite_file.write_bytes(converter.convert())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qkZD0VoVExe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fff98638-d4b4-4235-92b3-7eb78a15355f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp422umpa4/assets\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45733968"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Convert to TF Lite with quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "resnet_quantized_tflite_file = tflite_models_dir/\"resnet_v2_101_quantized.tflite\"\n",
        "resnet_quantized_tflite_file.write_bytes(converter.convert())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhOjeg1x9Knp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6b95879-7a20-46fa-fa28-ffbaf104e27a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root  24K Jun 16 20:23 /tmp/mnist_tflite_models/mnist_model_quant.tflite\n",
            "-rw-r--r-- 1 root root  83K Jun 16 20:23 /tmp/mnist_tflite_models/mnist_model.tflite\n",
            "-rw-r--r-- 1 root root  44M Jun 16 20:24 /tmp/mnist_tflite_models/resnet_v2_101_quantized.tflite\n",
            "-rw-r--r-- 1 root root 171M Jun 16 20:23 /tmp/mnist_tflite_models/resnet_v2_101.tflite\n"
          ]
        }
      ],
      "source": [
        "!ls -lh {tflite_models_dir}/*.tflite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqHLaqFMCjRZ"
      },
      "source": [
        "The model size reduces from 171 MB to 43 MB.\n",
        "The accuracy of this model on imagenet can be evaluated using the scripts provided for [TFLite accuracy measurement](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification).\n",
        "\n",
        "The optimized model top-1 accuracy is 76.8, the same as the floating point model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}