{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring inference time on a Raspberry Pi device"
   ],
   "id": "b8c9aa0c-2ae9-4ce5-a8d4-173fc3220d61"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect of quantization on the inference time of a model is dependent on the hardware environment, amongst other factors. Since quantization can be used to prepare deep learning models for deployment on edge devices, a comparison of the inference time of a model before and after quantization can be useful.\n",
    "\n",
    "In this notebook, we measure the inference times on a Raspberry Pi 4 with the objective of finding out the effectiveness of quantization in reducing inference time on edge devices."
   ],
   "id": "55a50882-aa0e-49b0-88c4-b99b7856fc84"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reservation"
   ],
   "id": "5b5de273-be2f-4d7d-be55-7c40c7c56021"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chi, os, time\n",
    "from chi import lease\n",
    "from chi import server\n",
    "from chi import container\n",
    "\n",
    "PROJECT_NAME = os.getenv('OS_PROJECT_NAME') # change this if you need to\n",
    "chi.use_site(\"CHI@Edge\")\n",
    "chi.set(\"project_name\", PROJECT_NAME)\n",
    "username = os.getenv('USER') # all exp resources will have this prefix"
   ],
   "id": "6976d4af-f388-4d66-a5c6-35a080839c71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE_TYPE = 'raspberrypi4-64'\n",
    "expname = \"edge-cpu\""
   ],
   "id": "d2dc918e-6a19-4cb6-ab0b-390fafe0d0b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "lease.add_device_reservation(res, machine_name=NODE_TYPE, count=1)\n",
    "\n",
    "start_date, end_date = lease.lease_duration(days=0, hours=10)\n",
    "# if you won't start right now - comment the line above, uncomment two lines below\n",
    "# start_date = '2024-04-02 15:24' # manually define to desired start time\n",
    "# end_date = '2024-04-03 01:00' # manually define to desired start time\n",
    "\n",
    "l = lease.create_lease(f\"{username}-{NODE_TYPE}\", res, start_date=start_date, end_date=end_date)\n",
    "l = lease.wait_for_active(l[\"id\"])  #Comment this line if the lease starts in the future"
   ],
   "id": "2d639259-9cdf-4a78-a580-7307e455d93e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue here, whether using a lease created just now or one created earlier\n",
    "l = lease.get_lease(f\"{username}-{NODE_TYPE}\")\n",
    "lease_id = l['id']"
   ],
   "id": "eea02520-3153-4df8-a489-43ddb23288b2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching a Container"
   ],
   "id": "2fd6aab7-6097-4756-9272-9366019f0237"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to launch a container!\n",
    "\n",
    "-   **Container** : A container is like a logical “box” that holds everything needed to run an application. It includes the application itself, along with all the necessary prerequisite software, files, and settings it needs to work properly.\n",
    "-   **Image** : An image is like a pre-packaged “starting point” for a container. On CHI@Edge, we can use any image that is built for the ARM64 architecture - e.g. anything on [this list](https://hub.docker.com/search?type=image&architecture=arm64&q=). In this example, we’re going to run a machine learning application written in Python, so we will use the `python:3.9-slim` image as a starting point for our container. This is a lightweight installation of the Debian Linux operating system with Python pre-installed.\n",
    "\n",
    "When we create the container, we could also specify some additional arguments:\n",
    "\n",
    "-   `workdir`: the “working directory” - location in the container’s filesystem from which any commands we specify will run.\n",
    "-   `exposed_ports`: if we run any applications inside the container that need to accept incoming requests from a network, we will need to export a “port” number for those incoming requests. Any requests to that port number will be forwarded to this container.\n",
    "-   `command`: if we want to execute a specific command immediately on starting the container, we can specify that as well.\n",
    "\n",
    "For this particular experiment, we’ll specify that port 22 - which is used for SSH access - should be exposed.\n",
    "\n",
    "Also, since we do not specify a `command` to run, we will further specify `interactive = True` - that it should open an interactive Python session - otherwise the container will immediately stop after it is started, because it has no “work” to do.\n",
    "\n",
    "First, we’ll specify the name for our container - we’ll include our username and the experiment name in the container name, so that it will be easy to identify our container in the CHI@Edge web interface."
   ],
   "id": "5ce79d2f-7823-45ae-8844-195ddf31e139"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a name for the container\n",
    "# Note that underscore characters _ are not allowed - we replace each _ with a -\n",
    "container_name = f\"{username}-{expname}\".replace(\"_\",\"-\")"
   ],
   "id": "c133d7e2-152e-478a-90d9-5f7e9b8ccf14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can create the container!"
   ],
   "id": "971f4391-2887-4c51-8177-1a97d39db991"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    my_container = container.create_container(\n",
    "        container_name,\n",
    "        image=\"python:3.9-slim\",\n",
    "        reservation_id=lease.get_device_reservation(lease_id),\n",
    "        interactive=True,\n",
    "        exposed_ports=[22],\n",
    "        platform_version=2,\n",
    "    )\n",
    "except RuntimeError as ex:\n",
    "    print(ex)\n",
    "    print(f\"Please stop and/or delete {container_name} and try again\")\n",
    "else:\n",
    "    print(f\"Successfully created container: {container_name}!\")"
   ],
   "id": "e6b84f2f-c2a4-489a-bd43-d14963458a47"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell waits for the container to be active - when it is, it will print some output related to the container state."
   ],
   "id": "be945ee9-f638-4408-a862-4312629c43f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait until container is ready to use\n",
    "container.wait_for_active(my_container.uuid)"
   ],
   "id": "ebdce553-7d2f-4cdf-8163-4584c7cc85ad"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the container is created, you should be able to see it and monitor its status on the [CHI@Edge web interface](https://chi.edge.chameleoncloud.org/project/container/containers). (If there was any problem while creating the container, you can also delete the container from that interface, in order to be able to try again.)"
   ],
   "id": "77b0fbe9-808e-442c-bc7b-cb8e18736371"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfering code files to the container"
   ],
   "id": "b5ffbf17-bfde-452f-aae9-094157b3996f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later in this notebook, we’ll repeatedly run the TFlite benchmark on the models and plot the results. The code file for repeatedly running the benchmark is in [this](https://github.com/AhmedFarrukh/DeepLearning-EdgeComputing/tree/main) git repository, which we need to transfer to the container.\n",
    "\n",
    "First, we will clone the repositoy to bring the relevant files onto the chameleon server and then transfer them to the container on our edge device."
   ],
   "id": "79f350f3-3831-4f75-b416-17b054e9393e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/AhmedFarrukh/DeepLearning-EdgeComputing.git"
   ],
   "id": "606c70b9-612e-4adc-971d-cb2d11a5699d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will upload the `code` directory from this repository to the `/root/` directory in the container."
   ],
   "id": "dba50e85-4d9b-4ede-9302-8fd153c3c529"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container.upload(my_container.uuid, \"./DeepLearning-EdgeComputing/code\", \"/root/\")"
   ],
   "id": "dce649b8-5223-43d0-8ca2-985ff60f9892"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can verify that the files were successfully transferred. The following cell should print the contents of the `code` directory from the repository."
   ],
   "id": "90effaa3-050b-42ba-904d-aa90e264c810"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(container.execute(my_container.uuid, 'ls -R /root/code')['output'])"
   ],
   "id": "f84182ba-f66b-473d-b058-66a597128253"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading files to the container"
   ],
   "id": "ddcefb1f-cd13-42cb-ace1-2171ff790689"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run the experiment, we need to download the models, as well as the TFlite benchmark."
   ],
   "id": "6a6f4e01-d2a2-4784-ad16-8e6976f97057"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first install `wget` and `gdown`. `wget` is a utility used to download files from the internet which we will use to downlaod the TFlite benchmark, and `gdown` is a tool to download files specifically from Google Drive, where the models are stored."
   ],
   "id": "4a92e493-48d0-4e35-9db7-3c6dfe47424b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container.execute(my_container.uuid, 'apt update')\n",
    "container.execute(my_container.uuid, 'apt -y install wget')"
   ],
   "id": "19854c77-addf-4e0f-b0ee-ffac21544b0f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container.execute(my_container.uuid, 'pip install gdown')"
   ],
   "id": "129411d4-28c5-40c2-9789-a8f723edc328"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can download the TFlite benchmark, storing it in `/root/benchmark` directory."
   ],
   "id": "330424ed-e21d-474c-8353-42c784c498bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container.execute(my_container.uuid, 'mkdir /root/benchmark')\n",
    "container.execute(my_container.uuid,'wget https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/linux_aarch64_benchmark_model -P /root/benchmark')"
   ],
   "id": "4c886c29-a520-4429-81c5-4e054e476caf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command should verify that the benchmark was correctly downloaded. We should be able to see a benchmark binary in the `/root/benchmark` directory."
   ],
   "id": "abb0ef29-08f7-49b7-b0e2-81e8c7731fcd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(container.execute(my_container.uuid, 'ls /root/benchmark')['output'])"
   ],
   "id": "699c6834-2d41-4ad0-9d7e-290d8ac7821f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to update the permissions of the benchmark binary and allow it to be executed."
   ],
   "id": "87e15f5f-05c4-43da-a7e7-613b218ab1e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container.execute(my_container.uuid,'chmod +x /root/benchmark/linux_aarch64_benchmark_model')"
   ],
   "id": "bd59554a-1644-41e5-9946-9d02d0817751"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need now is to download the models themselves. Using `gdown`, the models are downloaded from Google Drive and stored in the `/root/tflite_models` directory."
   ],
   "id": "8acc65c6-064d-4e8a-bc85-261fa9903493"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container.execute(my_container.uuid, 'mkdir /root/tflite_models')\n",
    "container.execute(my_container.uuid, 'gdown --folder https://drive.google.com/drive/folders/1OcJ9ceYg6ZWFJ4QMR0zznsw0KVeHPa4h -O /root/tflite_models')"
   ],
   "id": "4d9108dd-2478-400b-b55f-f42f01a179cc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command should verify that the models were correctly downloaded. In the `/root/tflite_models` directory, we should be able to see two versions of each model: original and quantized. Note that the original models are about four times larger in size than the quantized models."
   ],
   "id": "b72704b7-57b7-47de-b62e-4042655a0eef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(container.execute(my_container.uuid, 'ls -lR /root/tflite_models')['output'])"
   ],
   "id": "a22c8ae9-24e8-4cf2-8109-cdce92f45e1a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the benchmark"
   ],
   "id": "d878743a-79be-4838-9c86-5345c84cd8f8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the benchmark on the `tflite` models using the `run_benchmark` file in the `code` directory we transferred to the container earlier. For each model, the `run_benchmark` file runs the benchmark 10 times, storing the output in a file; the file name is the same as the model and the output files are stores in the `/root/results` directory. Once all models have been benchmarked, a file by the name of `completed` is created in the directory. In the next step, we will then parse through these output files, extract the relevant data and create plots.\n",
    "\n",
    "This step could take about 3 hours."
   ],
   "id": "23e1bd6c-5684-44e9-a426-ea6f4ad366c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container.execute(my_container.uuid, 'mkdir /root/results')\n",
    "container.execute(my_container.uuid, 'python /root/code/run_benchmark.py')"
   ],
   "id": "43a7f2bc-a5d1-4389-b2da-2d6e32165f18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ],
   "id": "314a01e3-82ec-4077-aa74-c8279c059b2a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we plot the results, let’s make sure that the python code has finished running. We can read the contents of the `/root/results`. If finished, the directory should contain one file for each model as well as a file named `completed`. If this is not the case, please wait for the code to finish executing."
   ],
   "id": "29635e8b-e2e0-4ac0-963d-53d661e2ecc5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(container.execute(my_container.uuid, 'ls /root/results')['output'])"
   ],
   "id": "d615255b-82c3-42cb-a0d7-c40efc97f67c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at one of the files to see what the output of the benchmark looks like."
   ],
   "id": "796eda37-bb99-4fa8-beac-80d639941426"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(container.execute(my_container.uuid, 'cat /root/results/MobileNet.txt')['output'])"
   ],
   "id": "2b0a9b27-4994-4a92-a757-687839056276"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s define all the models we tested on and the metrics that are reported by the benchmark:"
   ],
   "id": "1b7a3f15-e936-45c5-bc5d-3fd7de7eea4e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNames = [\"MobileNet\", \"InceptionV3\", \"ResNet50\", \"ResNet101\", \"ResNet152\", \"VGG16\", \"VGG19\"]\n",
    "metrics = [\"Init Time (ms)\", \"Init Inference (ms)\", \"First Inference (ms)\", \"Warmup Inference (ms)\", \"Avg Inference (ms)\", \"Memory Init (MB)\", \"Memory Overall (MB)\"]"
   ],
   "id": "6f4fa2ea-ca91-4771-982b-4e03c45ede41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the result of the benchmark is reported as text, we can define a parsing function to extract the data. The parsing function takes the output of the benchmark as an input and adds the results to a dictionary of metrics."
   ],
   "id": "2e3d13f7-3267-45b3-aace-b06f1a802a80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_benchmark_output(output, results):\n",
    "    \"\"\"\n",
    "    Parse benchmark output to extract model initialization times, inference timings, and memory footprint.\n",
    "    \"\"\"\n",
    "\n",
    "    # Regular expressions to match the required information\n",
    "    init_time_patterns = [\n",
    "        re.compile(r'INFO: Initialized session in (\\d+.\\d+)ms.'),\n",
    "        re.compile(r'INFO: Initialized session in (\\d+)ms.')\n",
    "    ]\n",
    "    inference_patterns = [\n",
    "        re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): ([\\d.e+]+), Inference \\(avg\\): ([\\d.e+]+)'),\n",
    "        re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): ([\\d.e+]+), Inference \\(avg\\): (\\d+)'),\n",
    "        re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): (\\d+.\\d+), Inference \\(avg\\): (\\d+.\\d+)'),\n",
    "        re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): (\\d+), Inference \\(avg\\): (\\d+.\\d+)'),\n",
    "        re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): (\\d+), Inference \\(avg\\): (\\d+)'),\n",
    "    ]\n",
    "    memory_patterns = [\n",
    "        re.compile(r'INFO: Memory footprint delta from the start of the tool \\(MB\\): init=(\\d+.\\d+) overall=(\\d+.\\d+)'),\n",
    "        re.compile(r'INFO: Memory footprint delta from the start of the tool \\(MB\\): init=(\\d+.\\d+) overall=(\\d+)'),\n",
    "        re.compile(r'INFO: Memory footprint delta from the start of the tool \\(MB\\): init=(\\d+) overall=(\\d+.\\d+)'),\n",
    "        re.compile(r'INFO: Memory footprint delta from the start of the tool \\(MB\\): init=(\\d+) overall=(\\d+)'),\n",
    "    ]\n",
    "    for line in output.split('\\n'):\n",
    "        # Match the initialization time\n",
    "        for pattern in init_time_patterns:\n",
    "            init_match = pattern.search(line)\n",
    "            if init_match:\n",
    "                results['Init Time (ms)'].append(float(init_match.group(1)))\n",
    "                break\n",
    "\n",
    "        # Match the inference timings\n",
    "        for pattern in inference_patterns:\n",
    "            inference_match = pattern.search(line)\n",
    "            if inference_match:\n",
    "                results[\"Init Inference (ms)\"].append(int(inference_match.group(1))/1000)\n",
    "                results[\"First Inference (ms)\"].append(int(inference_match.group(2))/1000)\n",
    "                results[\"Warmup Inference (ms)\"].append(float(inference_match.group(3))/1000)\n",
    "                results[\"Avg Inference (ms)\"].append(float(inference_match.group(4))/1000)\n",
    "                break\n",
    "\n",
    "        # Match the memory footprint\n",
    "        for pattern in memory_patterns:\n",
    "            memory_match = pattern.search(line)\n",
    "            if memory_match:\n",
    "              results['Memory Init (MB)'].append(float(memory_match.group(1)))\n",
    "              results['Memory Overall (MB)'].append(float(memory_match.group(2)))\n",
    "              break\n"
   ],
   "id": "b074aedb-0748-496f-800a-a7eac3ab3deb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define a Pandas Dataframe to store our results. Since we will be repeatedly running the benchmark to estimate the standard deviation of results as well, for each metric, we will define two columns - one for the mean and the other for the standard deviation."
   ],
   "id": "3bdc4ca1-55b3-4295-a7a0-c10e0ca9ad67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define model types (rows)\n",
    "rows = []\n",
    "for model in modelNames:\n",
    "  rows.append(model)\n",
    "  rows.append(model + \"_quant\")\n",
    "\n",
    "# Define columns\n",
    "cols = []\n",
    "for metric in metrics:\n",
    "  cols.append(metric)\n",
    "  cols.append(metric + \"_sd\")\n",
    "\n",
    "# Create an empty DataFrame\n",
    "finalResult = pd.DataFrame(index=rows, columns=cols)"
   ],
   "id": "061c017a-0edc-4b0f-b00d-80d0dc01452e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the functions and structures defined above to parse through the data and populate the dataframe with relevant metrics."
   ],
   "id": "11aaed8e-74ef-4fec-9c96-ad2ea355baf9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from collections import defaultdict\n",
    "from statistics import mean\n",
    "from statistics import stdev\n",
    "\n",
    "n = 10 #the number of times the benchmark is called for each model\n",
    "\n",
    "for modelName in rows:\n",
    "  print(modelName)\n",
    "  modelResults = defaultdict(list)\n",
    "  outputOriginal = container.execute(my_container.uuid, 'cat /root/results/' + modelName + '.txt')['output']\n",
    "  output = parse_benchmark_output(outputOriginal, modelResults)\n",
    "\n",
    "  for metric in metrics:\n",
    "    finalResult.loc[modelName, metric] = mean(modelResults[metric])\n",
    "    finalResult.loc[modelName, metric + \"_sd\"] = stdev(modelResults[metric])"
   ],
   "id": "4a5484e6-6689-47d5-9121-d6a7cc1fb235"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s have a look at the results."
   ],
   "id": "32de3200-d6ae-4b44-9268-6001a4bb97cf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(finalResult)"
   ],
   "id": "d1c7c2b7-223a-4d9c-bbe7-8ecd80c8cccc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s create a directory to store our results:"
   ],
   "id": "b9127c1d-b5bb-4cea-86aa-bcb1d4419915"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /work/DeepLearning-EdgeComputing/RaspberryPi4Results"
   ],
   "id": "599aed5f-8bb3-4f5b-8091-fcbe6cff5daa"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save our dataframe in the `/work/DeepLearning-EdgeComputing/RaspberryPi4Results` directory as a csv file for later reference."
   ],
   "id": "5f2dac1a-97b8-49e1-91e5-12c7f8497268"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalResult.to_csv(\"/work/DeepLearning-EdgeComputing/RaspberryPi4Results/finalResult.csv\")"
   ],
   "id": "5106f55d-3c99-41e5-96be-b265d36e7520"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can generate plots of the results."
   ],
   "id": "f410bcd6-c5d7-46d0-b71a-758a6f7fafec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "for metric in metrics:\n",
    "    means_orig = finalResult.loc[modelNames, metric].values\n",
    "    errors_orig = finalResult.loc[modelNames, metric + \"_sd\"].values\n",
    "    means_quant = finalResult.loc[[model + \"_quant\" for model in modelNames], metric].values\n",
    "    errors_quant = finalResult.loc[[model + \"_quant\" for model in modelNames], metric + \"_sd\"].values\n",
    "\n",
    "\n",
    "    n_groups = len(modelNames)\n",
    "    index = np.arange(n_groups)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.8\n",
    "\n",
    "    rects1 = plt.bar(index, means_orig, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     yerr=errors_orig,\n",
    "                     label='Original')\n",
    "\n",
    "    rects2 = plt.bar(index + bar_width, means_quant, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     yerr=errors_quant,\n",
    "                     label='Quantized')\n",
    "\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f'Bar Chart for {metric}')\n",
    "    plt.xticks(index + bar_width / 2, modelNames, rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot as an image\n",
    "    plt.savefig(\"/work/DeepLearning-EdgeComputing/RaspberryPi4Results/\" + metric + \".png\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ],
   "id": "969fd54c-5d64-46b7-802c-60c0edd095a5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the container"
   ],
   "id": "444229d1-b61e-4383-9633-96e6e2fb60a6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we should stop and delete our container so that others can create new containers using the same lease. To delete our container, we can run the following cell:"
   ],
   "id": "6de8dd55-e7bc-42db-9045-ba6bd4b270ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container.destroy_container(my_container.uuid)"
   ],
   "id": "900238f5-56f3-4bad-b49f-3a3d0573e646"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to delete the lease as well."
   ],
   "id": "b35cd263-48de-46a1-9e3a-18a7827cc1bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELETE = False #Default value is False to prevent any accidental deletes. Change it to True for deleting the resources\n",
    "\n",
    "if DELETE:\n",
    "    # delete lease\n",
    "    chi.lease.delete_lease(lease[\"id\"])"
   ],
   "id": "4921edb5-7968-4038-b507-186b8d20f7d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
