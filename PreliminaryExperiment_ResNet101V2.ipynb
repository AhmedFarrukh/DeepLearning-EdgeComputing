{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedFarrukh/DeepLearning-EdgeComputing/blob/main/PreliminaryExperiment_ResNet101V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ockh2T00gt4c"
      },
      "source": [
        "In this notebook, I will load the pretrained ResNet101V2 model, apply Post-training Dynamic Range Quantization, and then measure the approximate change in inference time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4w3mAeXjyjv"
      },
      "source": [
        "First, we import the neccessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0D1cnpMjxnE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTtHQiFPj2-y"
      },
      "source": [
        "Next, we load the ResNet101V2 Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNVt3xNSj-n2"
      },
      "outputs": [],
      "source": [
        "INPUT_IMG_SIZE = 224\n",
        "INPUT_IMG_SHAPE = (224, 224, 3)\n",
        "model = tf.keras.applications.ResNet101V2(\n",
        "  input_shape=INPUT_IMG_SHAPE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBfrb17NkF3C"
      },
      "source": [
        "First, we convert the model to TensorFlow Lite format, without any optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkxL8rYbkLJ8"
      },
      "outputs": [],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHPYTw2dkPrF"
      },
      "source": [
        "Next, we convert the model to TensorFlow Lite format, with Dynamic Range Quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhcHMKewkVaD"
      },
      "outputs": [],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "tflite_model_quant = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIHhHm9ikgoh"
      },
      "source": [
        "Write it out to tflite file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLWLeHsmkipk",
        "outputId": "6fa7a52b-c941-442c-bd84-529ad8f6a980"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45646584"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "import pathlib\n",
        "\n",
        "tflite_models_dir = pathlib.Path(\"/tmp/tflite_models/\")\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Save the unquantized/float model:\n",
        "tflite_model_file = tflite_models_dir/\"model.tflite\"\n",
        "tflite_model_file.write_bytes(tflite_model)\n",
        "# Save the quantized model:\n",
        "tflite_model_quant_file = tflite_models_dir/\"model_quant.tflite\"\n",
        "tflite_model_quant_file.write_bytes(tflite_model_quant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqb91z7PlKLO"
      },
      "source": [
        "Let's check the sizes of the tflite models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlllkLkglN8e",
        "outputId": "69d40d44-afcb-4b02-916a-39741cd7bd70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 214M\n",
            "-rw-r--r-- 1 root root  44M Jun 21 13:33 model_quant.tflite\n",
            "-rw-r--r-- 1 root root 171M Jun 21 13:33 model.tflite\n"
          ]
        }
      ],
      "source": [
        "!ls -lh {tflite_models_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOsxLHCimugT"
      },
      "source": [
        "Next, I upload a sample file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "U4QW_zGZmxRz",
        "outputId": "6a49c9e0-22cb-4cc4-a7b6-d9366370d1f0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8976f886-eb58-4b96-9792-785f66611a27\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8976f886-eb58-4b96-9792-785f66611a27\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving parrot.jpg to parrot (1).jpg\n",
            "User uploaded file \"parrot (1).jpg\" with length 596977 bytes\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "first_time = True\n",
        "if(first_time):\n",
        "  uploaded = files.upload()\n",
        "\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8oxK4dTm-HM"
      },
      "source": [
        "Next, use the model to run an inference on the sample image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75BU0vi5oR-A"
      },
      "outputs": [],
      "source": [
        "image_path = fn\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "img = img.resize((224, 224), Image.BICUBIC)\n",
        "input_data = np.array(img, dtype=np.float32) / 255.0\n",
        "input_data = input_data.reshape(1, 224, 224, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sODbFblhuB9R"
      },
      "source": [
        "Load the labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJapmhSPuDps"
      },
      "outputs": [],
      "source": [
        "url = tf.keras.utils.get_file(\n",
        "    'ImageNetLabels.txt',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
        "imagenet_labels = np.array(open(url).read().splitlines())[1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NToeQE4MoN_w"
      },
      "source": [
        "First, load the interpreters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0Z5ubc6nIG0"
      },
      "outputs": [],
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "interpreter_quant = tf.lite.Interpreter(model_path=str(tflite_model_quant_file))\n",
        "interpreter_quant.allocate_tensors()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1rg-5GMnY9J"
      },
      "source": [
        "Next, run the inference using the un-optimized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_-yFGb3nbhc",
        "outputId": "09e5114c-4b2f-4437-fa4d-f952d5583774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3248581886291504\n",
            "macaw\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "interpreter.set_tensor(input_index, input_data)\n",
        "interpreter.invoke()\n",
        "start_time = time.time()\n",
        "interpreter.invoke()\n",
        "print(time.time() - start_time)\n",
        "predictions = interpreter.get_tensor(output_index)\n",
        "print(imagenet_labels[np.argmax(predictions[0])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyJC_WxvsMWy"
      },
      "source": [
        "Finally, run the inference using the quantized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpee-7cksPuT",
        "outputId": "6e1cbb10-ed36-4906-bbe3-840fc933a573"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5406627655029297\n",
            "macaw\n"
          ]
        }
      ],
      "source": [
        "input_index = interpreter_quant.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter_quant.get_output_details()[0][\"index\"]\n",
        "\n",
        "interpreter_quant.set_tensor(input_index, input_data)\n",
        "interpreter_quant.invoke()\n",
        "start_time = time.time()\n",
        "interpreter_quant.invoke()\n",
        "print(time.time() - start_time)\n",
        "predictions = interpreter_quant.get_tensor(output_index)\n",
        "print(imagenet_labels[np.argmax(predictions[0])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYDFtXqOR-_S"
      },
      "source": [
        "Now, we use the validation set from the imagenette dataset to run inference on the two models. First, we prepare the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9mOXalUYSGcd",
        "outputId": "7f6792b0-d4b2-4475-fd6d-6e414428495e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-21 13:20:48--  https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.179.125, 52.217.141.192, 54.231.171.16, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.179.125|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 341663724 (326M) [application/x-tar]\n",
            "Saving to: ‘imagenette2-320.tgz’\n",
            "\n",
            "imagenette2-320.tgz  31%[=====>              ] 101.27M  16.7MB/s    eta 17s    ^C\n",
            "\n",
            "gzip: stdin: unexpected end of file\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz\n",
        "!tar -xzf imagenette2-320.tgz\n",
        "\n",
        "label_map = dict(\n",
        "    n01440764='tench',\n",
        "    n02102040='English springer',\n",
        "    n02979186='cassette player',\n",
        "    n03000684='chain saw',\n",
        "    n03028079='church',\n",
        "    n03394916='French horn',\n",
        "    n03417042='garbage truck',\n",
        "    n03425413='gas pump',\n",
        "    n03445777='golf ball',\n",
        "    n03888257='parachute'\n",
        ")\n",
        "\n",
        "\n",
        "def preprocess_image(file_path):\n",
        "    # Open the image file\n",
        "    img = Image.open(file_path).convert('RGB')\n",
        "\n",
        "    # Resize using BICUBIC interpolation\n",
        "    img = img.resize((224, 224), Image.BICUBIC)\n",
        "\n",
        "    # Convert to NumPy array and normalize the pixel values\n",
        "    input_data = np.array(img, dtype=np.float32) / 255.0\n",
        "\n",
        "    # Reshape for the model input\n",
        "    input_data = input_data.reshape(1, 224, 224, 3)\n",
        "    return input_data\n",
        "\n",
        "def load_and_preprocess_from_path_label(path):\n",
        "    return preprocess_image(path)\n",
        "\n",
        "# Set the directory path for validation data\n",
        "val_dir = 'imagenette2-320/val'\n",
        "\n",
        "# List dataset paths\n",
        "val_image_paths = [os.path.join(dp, f) for dp, dn, filenames in os.walk(val_dir) for f in filenames if os.path.splitext(f)[1].lower() in ['.png','.jpg','.jpeg']]\n",
        "\n",
        "# Create dataset from paths\n",
        "val_images = tf.data.Dataset.from_tensor_slices(val_image_paths)\n",
        "\n",
        "# Apply preprocessing using a map with the numpy function to ensure correct application\n",
        "val_images_preprocessed = val_images.map(lambda x: tf.numpy_function(load_and_preprocess_from_path_label, [x], tf.float32))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhHDO1AYTqew"
      },
      "source": [
        "Next, we carry out inference using the un-quantized model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "_NpVq7pyTtRE",
        "outputId": "40793721-65e5-4686-87ce-72d7fff7510e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ccde1cd03458>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0minfer_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m     \"\"\"\n\u001b[1;32m    940\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "infer_time = 0\n",
        "for image in val_images_preprocessed:\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  interpreter.set_tensor(input_index, image)\n",
        "  start_time = time.time()\n",
        "  interpreter.invoke()\n",
        "  infer_time += time.time() - start_time\n",
        "  predictions = interpreter.get_tensor(output_index)\n",
        "print(\"avg infer_time: \", infer_time/len(val_images_preprocessed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWKkznMkVTsi"
      },
      "source": [
        "Finally, we carry out inference using the quantized model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT7M2R2jVW5W"
      },
      "outputs": [],
      "source": [
        "infer_time = 0\n",
        "for image in val_images_preprocessed.take(1000):\n",
        "  input_index = interpreter_quant.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter_quant.get_output_details()[0][\"index\"]\n",
        "\n",
        "  interpreter_quant.set_tensor(input_index, image)\n",
        "  start_time = time.time()\n",
        "  interpreter_quant.invoke()\n",
        "  infer_time += time.time() - start_time\n",
        "  predictions = interpreter_quant.get_tensor(output_index)\n",
        "print(\"avg infer_time: \", infer_time/len(val_images_preprocessed))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to test the differences in models before and after quantization, the TFlite benchmark can be used. First, download the benchmark binary."
      ],
      "metadata": {
        "id": "3Mls_D_T6E8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /tmp/benchmark\n",
        "!wget https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/linux_x86-64_benchmark_model -P /tmp/benchmark\n",
        "!chmod +x /tmp/benchmark/linux_x86-64_benchmark_model"
      ],
      "metadata": {
        "id": "ob1aefXJ6WC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07390a39-6c77-45c3-d675-17197a29e1f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/tmp/benchmark’: File exists\n",
            "--2024-06-21 13:38:51--  https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/linux_x86-64_benchmark_model\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.204.207, 64.233.187.207, 64.233.188.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.204.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6237624 (5.9M) [application/octet-stream]\n",
            "Saving to: ‘/tmp/benchmark/linux_x86-64_benchmark_model.2’\n",
            "\n",
            "linux_x86-64_benchm 100%[===================>]   5.95M  5.72MB/s    in 1.0s    \n",
            "\n",
            "2024-06-21 13:38:52 (5.72 MB/s) - ‘/tmp/benchmark/linux_x86-64_benchmark_model.2’ saved [6237624/6237624]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, benchmark the original model:"
      ],
      "metadata": {
        "id": "Cg_qbCH87gL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/tmp/benchmark/linux_x86-64_benchmark_model \\\n",
        "  --graph=/tmp/tflite_models/model.tflite \\\n",
        "  --num_threads=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aT85vpa7kFJ",
        "outputId": "aa5d6cbf-2d93-45f3-fdca-5c33f37f7650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: STARTING!\n",
            "INFO: Log parameter values verbosely: [0]\n",
            "INFO: Num threads: [1]\n",
            "INFO: Graph: [/tmp/tflite_models/model.tflite]\n",
            "INFO: Signature to run: []\n",
            "INFO: #threads used for CPU inference: [1]\n",
            "INFO: Loaded model /tmp/tflite_models/model.tflite\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: The input model file size (MB): 178.341\n",
            "INFO: Initialized session in 1231.85ms.\n",
            "INFO: Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\n",
            "INFO: count=1 curr=511374\n",
            "\n",
            "INFO: Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\n",
            "INFO: count=50 first=515025 curr=286101 min=284629 max=515025 avg=334084 std=78671\n",
            "\n",
            "INFO: Inference timings in us: Init: 1231851, First inference: 511374, Warmup (avg): 511374, Inference (avg): 334084\n",
            "INFO: Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\n",
            "INFO: Memory footprint delta from the start of the tool (MB): init=0 overall=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, benchmark the quantified model:"
      ],
      "metadata": {
        "id": "fZqCeRMv90K7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/tmp/benchmark/linux_x86-64_benchmark_model \\\n",
        "  --graph=/tmp/tflite_models/model_quant.tflite \\\n",
        "  --num_threads=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlKHjuwV94EO",
        "outputId": "526fd17e-7c6c-40c3-b1ef-9d441d5480e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: STARTING!\n",
            "INFO: Log parameter values verbosely: [0]\n",
            "INFO: Num threads: [1]\n",
            "INFO: Graph: [/tmp/tflite_models/model_quant.tflite]\n",
            "INFO: Signature to run: []\n",
            "INFO: #threads used for CPU inference: [1]\n",
            "INFO: Loaded model /tmp/tflite_models/model_quant.tflite\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: The input model file size (MB): 45.6466\n",
            "INFO: Initialized session in 144.042ms.\n",
            "INFO: Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\n",
            "INFO: count=2 first=289289 curr=278135 min=278135 max=289289 avg=283712 std=5577\n",
            "\n",
            "INFO: Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\n",
            "INFO: count=50 first=285218 curr=274631 min=268874 max=455319 avg=308401 std=61370\n",
            "\n",
            "INFO: Inference timings in us: Init: 144042, First inference: 289289, Warmup (avg): 283712, Inference (avg): 308401\n",
            "INFO: Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\n",
            "INFO: Memory footprint delta from the start of the tool (MB): init=0 overall=0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjRBSU1Sgy1TeznZLXFfEt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}