{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Inference Times"
   ],
   "id": "1a82bfd3-4320-485f-9841-e35988403f27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we measure the inference times and memory footprint of 7 popular Convolutional Neural Network (CNN) models and their quantized versions. The CNN models are: MobileNet, InceptionV3, Resnet50, ResNet101, ResNet152, VGG16, VGG19.\n",
    "\n",
    "The quantized models were created by applying [Post-training Dynamic Range Quantization](https://www.tensorflow.org/lite/performance/post_training_quantization), which converts model weights from floating point numbers to 8-bit fixed width numbers.\n",
    "\n",
    "Both the original models and their quantized versions are of tflite format, and were uploaded to [Google Drive](https://drive.google.com/drive/folders/1OcJ9ceYg6ZWFJ4QMR0zznsw0KVeHPa4h?usp=drive_link).\n",
    "\n",
    "The benchmarking of models is achieved by using the official [TFlite benchmark](https://www.tensorflow.org/lite/performance/measurement) which measures the following metrics:  \n",
    "- Initialization time  \n",
    "- Inference time of warmup state  \n",
    "- Inference time of steady state  \n",
    "- Memory usage during initialization time  \n",
    "- Overall memory usage\n",
    "\n",
    "The benchmark generates a series of random inputs, runs the models and aggregates the results to report the aforementioned metrics."
   ],
   "id": "233c32fd-df47-4c6b-8f60-6a46dfc61a56"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, let’s check the specifications of our hardware environment."
   ],
   "id": "41d73e01-67c3-47e3-a87a-17cbd652eb07"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu"
   ],
   "id": "579862f3-e7d5-4dfd-87cd-6ca1ca795aa1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s define the CNN models we will be using."
   ],
   "id": "ddc25523-33bd-41fe-ad4b-35efff3e24a9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNames = [\"MobileNet\", \"InceptionV3\", \"ResNet50\", \"ResNet101\", \"ResNet152\", \"VGG16\", \"VGG19\"]"
   ],
   "id": "92444d62-9251-4679-923f-193159217081"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download the models from the Google Drive using `gdown`. If you want to download your own set of models, you can modify the google drive link below. In this case, we download the models to the `./tflite_models` directory."
   ],
   "id": "5c1c7937-a420-407e-9018-c8cae1ccf8fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "gdown.download_folder('https://drive.google.com/drive/folders/1OcJ9ceYg6ZWFJ4QMR0zznsw0KVeHPa4h')"
   ],
   "id": "5ebb071f-5a36-48bd-9bba-478e34dd0037"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that the models were correctly loaded by listing the files in the `./tflite_models directory`. Note that there should be two `.tflite` files for each model: an original and a quantized version. The size of the quantized models should be about four times smaller than the size of the corresponding original model."
   ],
   "id": "97953912-9602-41dc-8ea3-99a12cb0c41b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l ./tflite_models"
   ],
   "id": "a270e3b3-0382-4915-a603-dab90b1f6db3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download the TFlite benchmark which we will use to measure inference times and memory footprint. More details about the benchmark can be found on the [tensorflow website](https://www.tensorflow.org/lite/performance/measurement). Note that the benchmark is specific to the architecture type (such as x86 or ARM), and the appropriate benchmark binary must be downloaded. Below, the benchmark is loaded for an x86-64 type architecture.\n",
    "\n",
    "The benchmark is downloaded to the `./benchmark` directory, and its permissions are then updated to allow it to be executed."
   ],
   "id": "74fe4aae-a84c-4ea8-97fa-6c85ba352c0f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./benchmark\n",
    "!wget https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/linux_x86-64_benchmark_model -P ./benchmark\n",
    "!chmod +x ./benchmark/linux_x86-64_benchmark_model"
   ],
   "id": "5ac0f0af-33bc-412d-8812-071269fbe04a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s run the benchmark on the MobileNet_quant model and note the output."
   ],
   "id": "2c9c298c-d50d-4274-aaa6-bbc1b5b6d521"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./benchmark/linux_x86-64_benchmark_model \\\n",
    "      --graph=./tflite_models/MobileNet_quant.tflite \\\n",
    "      --num_threads=1"
   ],
   "id": "ecc87fff-5532-4591-a3c7-542fc1c7baea"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s define all the metrics that are reported by the benchmark:"
   ],
   "id": "92b990fb-bfda-4332-9be6-5315687eb6d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"Init Time (ms)\", \"Init Inference (ms)\", \"First Inference (ms)\", \"Warmup Inference (ms)\", \"Avg Inference (ms)\", \"Memory Init (MB)\", \"Memory Overall (MB)\"]"
   ],
   "id": "0c361d68-40d9-471c-9681-9adea78fd3ed"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the result of the benchmark is reported as text on the console, we can define a parsing function to extract the data. The parsing function takes the output of the benchmark as an input and adds the results to a dictionary of metrics."
   ],
   "id": "34938c8a-061b-4bb9-a38a-2b19958e516f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_benchmark_output(output, results):\n",
    "    \"\"\"\n",
    "    Parse benchmark output to extract model initialization times, inference timings, and memory footprint.\n",
    "    \"\"\"\n",
    "\n",
    "    # Regular expressions to match the required information\n",
    "    init_time_patterns = [\n",
    "        re.compile(r'INFO: Initialized session in (\\d+.\\d+)ms.'),\n",
    "        re.compile(r'INFO: Initialized session in (\\d+)ms.')\n",
    "    ]\n",
    "    inference_patterns = [\n",
    "        re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): ([\\d.e+]+), Inference \\(avg\\): ([\\d.e+]+)'),\n",
    "        re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): ([\\d.e+]+), Inference \\(avg\\): (\\d+)'),\n",
    "        re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): (\\d+.\\d+), Inference \\(avg\\): (\\d+.\\d+)'),\n",
    "        re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): (\\d+), Inference \\(avg\\): (\\d+.\\d+)'),\n",
    "        re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): (\\d+), Inference \\(avg\\): (\\d+)'),\n",
    "    ]\n",
    "    memory_patterns = [\n",
    "        re.compile(r'INFO: Memory footprint delta from the start of the tool \\(MB\\): init=(\\d+.\\d+) overall=(\\d+.\\d+)'),\n",
    "        re.compile(r'INFO: Memory footprint delta from the start of the tool \\(MB\\): init=(\\d+.\\d+) overall=(\\d+)'),\n",
    "        re.compile(r'INFO: Memory footprint delta from the start of the tool \\(MB\\): init=(\\d+) overall=(\\d+.\\d+)'),\n",
    "        re.compile(r'INFO: Memory footprint delta from the start of the tool \\(MB\\): init=(\\d+) overall=(\\d+)'),\n",
    "    ]\n",
    "    for line in output.split('\\n'):\n",
    "        # Match the initialization time\n",
    "        for pattern in init_time_patterns:\n",
    "            init_match = pattern.search(line)\n",
    "            if init_match:\n",
    "                results['Init Time (ms)'].append(float(init_match.group(1)))\n",
    "                break\n",
    "\n",
    "        # Match the inference timings\n",
    "        for pattern in inference_patterns:\n",
    "            inference_match = pattern.search(line)\n",
    "            if inference_match:\n",
    "                results[\"Init Inference (ms)\"].append(int(inference_match.group(1))/1000)\n",
    "                results[\"First Inference (ms)\"].append(int(inference_match.group(2))/1000)\n",
    "                results[\"Warmup Inference (ms)\"].append(float(inference_match.group(3))/1000)\n",
    "                results[\"Avg Inference (ms)\"].append(float(inference_match.group(4))/1000)\n",
    "                break\n",
    "\n",
    "        # Match the memory footprint\n",
    "        for pattern in memory_patterns:\n",
    "            memory_match = pattern.search(line)\n",
    "            if memory_match:\n",
    "              results['Memory Init (MB)'].append(float(memory_match.group(1)))\n",
    "              results['Memory Overall (MB)'].append(float(memory_match.group(2)))\n",
    "              break\n"
   ],
   "id": "adedc99c-f8cc-4334-893d-d660a7fd0629"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define a Pandas Dataframe to store our results. Since we will be repeatedly running the benchmark to estimate the standard deviation of results as well, for each metric we will define two columns - one for the mean and the other for the standard deviation."
   ],
   "id": "9a5ebe35-c154-41dc-bf7e-4a0ef3922ca9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define model types (rows)\n",
    "rows = []\n",
    "for model in modelNames:\n",
    "  rows.append(model)\n",
    "  rows.append(model + \"_quant\")\n",
    "\n",
    "# Define columns\n",
    "cols = []\n",
    "for metric in metrics:\n",
    "  cols.append(metric)\n",
    "  cols.append(metric + \"_sd\")\n",
    "\n",
    "# Create an empty DataFrame\n",
    "finalResult = pd.DataFrame(index=rows, columns=cols)"
   ],
   "id": "cec85165-b1bf-4a00-a266-f1f355188d81"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the benchmark repeatedly and average the results. For each model, we repeatedly run the benchmark, and parse the output from the benchmark. After `n` trials, the mean and standard deviation of the metrics is added to the `finalResult` dataframe defined in the last step."
   ],
   "id": "d92b9406-0550-4fa0-9388-e1e45e2b1ff8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from collections import defaultdict\n",
    "from statistics import mean\n",
    "from statistics import stdev\n",
    "\n",
    "n = 10 #the number of times the benchmark is called for each model\n",
    "\n",
    "for modelName in rows:\n",
    "  print(modelName)\n",
    "  modelResults = defaultdict(list)\n",
    "  for i in range(n):\n",
    "    outputOriginal = subprocess.check_output(\"./benchmark/linux_x86-64_benchmark_model \\\n",
    "      --graph=./tflite_models/\" + modelName +\".tflite\"+\" \\\n",
    "      --num_threads=1\", shell=True)\n",
    "    outputOriginal = outputOriginal.decode('utf-8')\n",
    "    output = parse_benchmark_output(outputOriginal, modelResults)\n",
    "\n",
    "  for metric in metrics:\n",
    "    finalResult.loc[modelName, metric] = mean(modelResults[metric])\n",
    "    finalResult.loc[modelName, metric + \"_sd\"] = stdev(modelResults[metric])"
   ],
   "id": "2924e7f2-3a49-464e-8bbb-7965b4efbd35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s have a look at the results."
   ],
   "id": "1d003424-68e6-47dc-98f4-0aa89656c2e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(finalResult)"
   ],
   "id": "6e781635-2b22-48ed-af02-8e81c8bacfc1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s create a directory to store the plots from our data:"
   ],
   "id": "82bdb7d0-9c42-49d4-ae3f-5fcd7ff435d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./plots"
   ],
   "id": "b93dddc7-8532-44e4-bdf4-d81248f835c7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can generate plots of the results."
   ],
   "id": "9d5e029f-e082-41d6-9848-4fec87d86039"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "for metric in metrics:\n",
    "    means_orig = finalResult.loc[modelNames, metric].values\n",
    "    errors_orig = finalResult.loc[modelNames, metric + \"_sd\"].values\n",
    "    means_quant = finalResult.loc[[model + \"_quant\" for model in modelNames], metric].values\n",
    "    errors_quant = finalResult.loc[[model + \"_quant\" for model in modelNames], metric + \"_sd\"].values\n",
    "\n",
    "\n",
    "    n_groups = len(modelNames)\n",
    "    index = np.arange(n_groups)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.8\n",
    "\n",
    "    rects1 = plt.bar(index, means_orig, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     yerr=errors_orig,\n",
    "                     label='Original')\n",
    "\n",
    "    rects2 = plt.bar(index + bar_width, means_quant, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     yerr=errors_quant,\n",
    "                     label='Quantized')\n",
    "\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f'Bar Chart for {metric}')\n",
    "    plt.xticks(index + bar_width / 2, modelNames, rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot as an image\n",
    "    plt.savefig(\"./plots\" + metric + \"_bar_chart.png\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ],
   "id": "dc0b84df-2185-470c-b19d-386fe8871d74"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
