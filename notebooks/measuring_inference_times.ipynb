{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Inference Times"
   ],
   "id": "adc0eb95-3bf3-423d-9319-be5ba9d694b6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we measure the inference times and memory footprint of 7 popular Convolutional Neural Network (CNN) models and their quantized versions. The CNN models are: MobileNet, InceptionV3, Resnet50, ResNet101, ResNet152, VGG16, VGG19.\n",
    "\n",
    "The quantized models were created by applying [Post-training Dynamic Range Quantization](https://www.tensorflow.org/lite/performance/post_training_quantization), which converts model weights from floating point numbers to 8-bit fixed width numbers.\n",
    "\n",
    "Both the original models and their quantized versions are of tflite format, and were uploaded to [Google Drive](https://drive.google.com/drive/folders/1OcJ9ceYg6ZWFJ4QMR0zznsw0KVeHPa4h?usp=drive_link).\n",
    "\n",
    "The benchmarking of models is achieved by using the official [TFlite benchmark](https://www.tensorflow.org/lite/performance/measurement) which measures the following metrics:  \n",
    "- Initialization time  \n",
    "- Inference time of warmup state  \n",
    "- Inference time of steady state  \n",
    "- Memory usage during initialization time  \n",
    "- Overall memory usage\n",
    "\n",
    "The benchmark generates a series of random inputs, runs the models and aggregates the results to report the aforementioned metrics."
   ],
   "id": "9b0b8a82-f586-4c29-9b3f-b2809539a4b0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, let’s check the specifications of our hardware environment. The `lscpu` utility in Linux can be used to learn more about the CPU architecture. Amongst details to notice are the BogoMIPS value and clock speed which are both measurements of CPU speed; BogoMIPS (“*Bog*us” *M*illions of *I*nstructions per *S*econd) is calculated by the Linux kernel whereas clock speed is reported by the hardware manufacturer. Also pay attention to the flags, and see if there are any special deep learning optimizations, such as the AVX-512 VNNI isntruction set which accelerates convolutional neural networks."
   ],
   "id": "fc9fe8bd-6aaf-4104-816e-ae2f7d2b89ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu"
   ],
   "id": "d4877ceb-868e-4dc4-80a7-4fc552b6bc77"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s particularly important to take note of the architecture type (x86-64/ARM64 etc.) since the benchmark binary we will use later is specific to the architecture type. The `platform` library in python can be used to find out more about the architecture, and in our case, to store the architecture type in a variable for later use."
   ],
   "id": "81fb23df-7129-4148-88ba-32d4f5f188e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "arch_type = platform.machine().replace(\"_\", \"-\")\n",
    "print(arch_type)"
   ],
   "id": "2eb91678-a722-4b5f-826e-b0549ad6c1ad"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s define the CNN models we will be using."
   ],
   "id": "ec684c97-0671-4117-bb50-bced402c1f0f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNames = [\"MobileNet\", \"InceptionV3\", \"ResNet50\", \"ResNet101\", \"ResNet152\", \"VGG16\", \"VGG19\"]"
   ],
   "id": "dee4d2dd-de69-4094-85ae-3ad1e0d52dd5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download the models from the Google Drive using `gdown`. If you want to download your own set of models, you can modify the google drive link below. In this case, the `tflite_models` folder is downloaded from Google Drive and we will be able to access the models in the `./tflite_models` directory."
   ],
   "id": "d706e9fb-402a-468e-b163-e2672a6c4c99"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "gdown.download_folder('https://drive.google.com/drive/folders/1OcJ9ceYg6ZWFJ4QMR0zznsw0KVeHPa4h')"
   ],
   "id": "08fef7a1-6f14-4737-ba4e-7963ff525492"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that the models were correctly loaded by listing the files in the `./tflite_models` directory. Note that there should be two `.tflite` files for each model: an original and a quantized version. The size of the quantized models should be about four times smaller than the size of the corresponding original model."
   ],
   "id": "8c24cd6b-42ee-4e05-a44d-2b05c25e2481"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l ./tflite_models"
   ],
   "id": "f9b554b7-17cc-4a5e-95ab-de7dc3835cbc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download the TFlite benchmark which we will use to measure inference times and memory footprint. More details about the benchmark can be found on the [tensorflow website](https://www.tensorflow.org/lite/performance/measurement). Note that the benchmark is specific to the architecture type (such as x86 or ARM), and the appropriate benchmark binary must be downloaded; therefore, we will concatenate the `arch_type` we defined earlier to the download link.\n",
    "\n",
    "The benchmark is downloaded and its permissions are then updated to allow it to be executed."
   ],
   "id": "020a5900-6cbf-4273-ab6a-0fd81fd00f6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/linux_{arch_type}_benchmark_model -O benchmark\n",
    "!chmod +x ./benchmark"
   ],
   "id": "7a98c8a4-354b-49d6-8275-a2f198ab964f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s run the benchmark on the MobileNet_quant model and note the output."
   ],
   "id": "8eaf5591-71c8-424b-9671-58664f9c2d03"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./benchmark \\\n",
    "      --graph=./tflite_models/MobileNet_quant.tflite \\\n",
    "      --num_threads=1"
   ],
   "id": "148ee0c5-c32a-4fc3-bd82-5da1da1c6617"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s define all the metrics that are reported by the benchmark:"
   ],
   "id": "c5926a54-d6b0-49e3-bb75-bf9fd2b2dc87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"Init Time (ms)\", \"Init Inference (ms)\", \"First Inference (ms)\", \"Warmup Inference (ms)\", \"Avg Inference (ms)\", \"Memory Init (MB)\", \"Memory Overall (MB)\"]"
   ],
   "id": "bd6ea209-2461-4776-973e-63165b9cca33"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the result of the benchmark is reported as text on the console, we can define a parsing function to extract the data. The parsing function takes the output of the benchmark as an input and adds the results to a dictionary of metrics.\n",
    "\n",
    "The function employs regular expressions to extract key performance metrics from the output logs. It defines specific patterns and attempts to match these against the output logs. When a match is identified, the corresponding metrics are stored in a dictionary provided to the function. The metrics, as defined earlier, serve as the keys in this dictionary. Each key is associated with an array that contains the values reported for that metric, allowing for organized and accessible data retrieval for further analysis."
   ],
   "id": "0bb08c16-b262-48c8-91b1-d4c7832fbb68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_benchmark_output(output, results):\n",
    "    \"\"\"\n",
    "    Parse benchmark output to extract model initialization times, inference timings, and memory footprint.\n",
    "    \"\"\"\n",
    "\n",
    "    # Regular expressions to match the required information\n",
    "    init_time_patterns = [\n",
    "        re.compile(r'INFO: Initialized session in (\\d+.\\d+)ms.'),\n",
    "        re.compile(r'INFO: Initialized session in (\\d+)ms.')\n",
    "    ]\n",
    "    inference_patterns = [\n",
    "        re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): ([\\d.e+]+), Inference \\(avg\\): ([\\d.e+]+)'),\n",
    "        re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): ([\\d.e+]+), Inference \\(avg\\): (\\d+)'),\n",
    "        re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): (\\d+.\\d+), Inference \\(avg\\): (\\d+.\\d+)'),\n",
    "        re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): (\\d+), Inference \\(avg\\): (\\d+.\\d+)'),\n",
    "        re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): (\\d+), Inference \\(avg\\): (\\d+)'),\n",
    "    ]\n",
    "    memory_patterns = [\n",
    "        re.compile(r'INFO: Memory footprint delta from the start of the tool \\(MB\\): init=(\\d+.\\d+) overall=(\\d+.\\d+)'),\n",
    "        re.compile(r'INFO: Memory footprint delta from the start of the tool \\(MB\\): init=(\\d+.\\d+) overall=(\\d+)'),\n",
    "        re.compile(r'INFO: Memory footprint delta from the start of the tool \\(MB\\): init=(\\d+) overall=(\\d+.\\d+)'),\n",
    "        re.compile(r'INFO: Memory footprint delta from the start of the tool \\(MB\\): init=(\\d+) overall=(\\d+)'),\n",
    "    ]\n",
    "    for line in output.split('\\n'):\n",
    "        # Match the initialization time\n",
    "        for pattern in init_time_patterns:\n",
    "            init_match = pattern.search(line)\n",
    "            if init_match:\n",
    "                results['Init Time (ms)'].append(float(init_match.group(1)))\n",
    "                break\n",
    "\n",
    "        # Match the inference timings\n",
    "        for pattern in inference_patterns:\n",
    "            inference_match = pattern.search(line)\n",
    "            if inference_match:\n",
    "                results[\"Init Inference (ms)\"].append(int(inference_match.group(1))/1000)\n",
    "                results[\"First Inference (ms)\"].append(int(inference_match.group(2))/1000)\n",
    "                results[\"Warmup Inference (ms)\"].append(float(inference_match.group(3))/1000)\n",
    "                results[\"Avg Inference (ms)\"].append(float(inference_match.group(4))/1000)\n",
    "                break\n",
    "\n",
    "        # Match the memory footprint\n",
    "        for pattern in memory_patterns:\n",
    "            memory_match = pattern.search(line)\n",
    "            if memory_match:\n",
    "              results['Memory Init (MB)'].append(float(memory_match.group(1)))\n",
    "              results['Memory Overall (MB)'].append(float(memory_match.group(2)))\n",
    "              break\n"
   ],
   "id": "d67fd508-139d-4b38-9a2e-def02896f814"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define a Pandas Dataframe to store our results. Since we will be repeatedly running the benchmark to estimate the standard deviation of results as well, for each metric we will define two columns - one for the mean and the other for the standard deviation."
   ],
   "id": "9c3fbc57-c8fd-4002-9bf4-df72ea5fa735"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define model types (rows)\n",
    "rows = []\n",
    "for model in modelNames:\n",
    "  rows.append(model)\n",
    "  rows.append(model + \"_quant\")\n",
    "\n",
    "# Define columns\n",
    "cols = []\n",
    "for metric in metrics:\n",
    "  cols.append(metric)\n",
    "  cols.append(metric + \"_sd\")\n",
    "\n",
    "# Create an empty DataFrame\n",
    "finalResult = pd.DataFrame(index=rows, columns=cols)"
   ],
   "id": "b4591141-eec5-41f6-9786-f9a80feebd0d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the benchmark repeatedly and average the results. For each model, we repeatedly run the benchmark, and parse the output from the benchmark. After `n` trials, the mean and standard deviation of the metrics is added to the `finalResult` dataframe defined in the last step."
   ],
   "id": "da2ca45e-2d72-412c-b786-50039d852f40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from collections import defaultdict\n",
    "from statistics import mean\n",
    "from statistics import stdev\n",
    "\n",
    "n = 10 #the number of times the benchmark is called for each model\n",
    "\n",
    "for modelName in rows:\n",
    "  print(modelName)\n",
    "  modelResults = defaultdict(list)\n",
    "  for i in range(n):\n",
    "    outputOriginal = subprocess.check_output(\"./benchmark \\\n",
    "      --graph=./tflite_models/\" + modelName +\".tflite\"+\" \\\n",
    "      --num_threads=1\", shell=True)\n",
    "    outputOriginal = outputOriginal.decode('utf-8')\n",
    "    output = parse_benchmark_output(outputOriginal, modelResults)\n",
    "\n",
    "  for metric in metrics:\n",
    "    finalResult.loc[modelName, metric] = mean(modelResults[metric])\n",
    "    finalResult.loc[modelName, metric + \"_sd\"] = stdev(modelResults[metric])"
   ],
   "id": "353af0c3-c81b-4a8f-9632-088adb9160dd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s have a look at the results."
   ],
   "id": "ffbe00d9-0e4c-45f0-86a7-1b191de909ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(finalResult)"
   ],
   "id": "7c977fd7-0942-4aca-a899-c60bd254d662"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s create a directory to store the results from our experiment."
   ],
   "id": "2218a108-0757-4ee6-ab6a-9055b62e7124"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./results"
   ],
   "id": "79b5f644-4f61-4c6d-a0a7-6299feb3b029"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s convert the `finalResult` dataframe to a csv file and store it in the `./results` directory, allowing us to download that data for later use."
   ],
   "id": "887b74d8-c455-4d31-8c27-8c3e6c213d50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalResult.to_csv(\"./results/finalResult.csv\")"
   ],
   "id": "414e9db9-e3be-4be9-aef8-196ef6eabe93"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can generate plots of the results. We will display the bars for the original and quantized versions of each model side-by-side to facilitate easy comparison. Error bars, representing +/- one standard deviation, will also be included to provide an estimate of the variation."
   ],
   "id": "e318d958-5689-4510-bf1b-f2a178378cd1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "for metric in metrics:\n",
    "    means_orig = finalResult.loc[modelNames, metric].values\n",
    "    errors_orig = finalResult.loc[modelNames, metric + \"_sd\"].values\n",
    "    means_quant = finalResult.loc[[model + \"_quant\" for model in modelNames], metric].values\n",
    "    errors_quant = finalResult.loc[[model + \"_quant\" for model in modelNames], metric + \"_sd\"].values\n",
    "\n",
    "\n",
    "    n_groups = len(modelNames)\n",
    "    index = np.arange(n_groups)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.8\n",
    "\n",
    "    rects1 = plt.bar(index, means_orig, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     yerr=errors_orig,\n",
    "                     label='Original')\n",
    "\n",
    "    rects2 = plt.bar(index + bar_width, means_quant, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     yerr=errors_quant,\n",
    "                     label='Quantized')\n",
    "\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f'Bar Chart for {metric}')\n",
    "    plt.xticks(index + bar_width / 2, modelNames, rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot as an image\n",
    "    plt.savefig(\"./results/\" + metric + \".png\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ],
   "id": "a15086f3-6d91-4ca6-a51c-1a87ba7b037e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you look through the plots, pay particular attention to the Average Inference Time plot, and note if the quantization led to a decrease in inference time, and if so, by how much.\n",
    "\n",
    "It is also interesting to note that sometimes even if the average inference time is greater for the quantized models, quantization might reduce other sources of latency, such as initialization time."
   ],
   "id": "b2b5f3f1-d03b-4bee-923b-c36309d01458"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
