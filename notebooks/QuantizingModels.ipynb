{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxUhwzI8rz0tJSIW+HYeOH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedFarrukh/DeepLearning-EdgeComputing/blob/main/notebooks/QuantizingModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook illustrates the process of applying Quantization to Deep Learning Models.\n",
        "\n",
        "Quantization, a technique that compresses models by converting parameters from floating point to fixed width numbers, can be useful to prepare models for deployment on edge devices. It significantly decreases the size of models and may improve inference times as well. In this notebook, [Post-training Dynamic Range Quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) is implemented, which converts model weights from floating point to integers with 8-bit precision.\n",
        "\n",
        "The technique is applied on popular Convolutional Neural Networks: MobileNet, InceptionV3, Resnet50, ResNet101, ResNet152, VGG16, VGG19.  \n"
      ],
      "metadata": {
        "id": "ruLYUSmf_rWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, the neccessary libraries are imported."
      ],
      "metadata": {
        "id": "7BKcLZQjZx7x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IK96M9Ah_ldV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import numpy as np\n",
        "import pathlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A list of the names of the models to be quantized is defined."
      ],
      "metadata": {
        "id": "G8D77ddmZ1M4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelNames = [\"MobileNet\", \"InceptionV3\", \"ResNet50\", \"ResNet101\", \"ResNet152\", \"VGG16\", \"VGG19\"]"
      ],
      "metadata": {
        "id": "AJp4asrE_uPs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is loaded using the Keras API. One instance of the model is converted to tflite format without any optimization; the other instance is optimized using Dynamic Range Quantization.\n",
        "\n",
        "Both the original and the quantized versions of the models are stored in the /root/tflite_models directory."
      ],
      "metadata": {
        "id": "6W1BBYLWZ8hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for modelName in modelNames:\n",
        "  model_class = getattr(tf.keras.applications, modelName)\n",
        "  model = model_class(weights='imagenet')\n",
        "\n",
        "  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "  tflite_model = converter.convert()\n",
        "\n",
        "  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  tflite_model_quant = converter.convert()\n",
        "\n",
        "  tflite_models_dir = pathlib.Path(\"/root/tflite_models/\")\n",
        "  tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "  # Save the unquantized/float model:\n",
        "  tflite_model_file = tflite_models_dir/(modelName+\".tflite\")\n",
        "  tflite_model_file.write_bytes(tflite_model)\n",
        "  # Save the quantized model:\n",
        "  tflite_model_quant_file = tflite_models_dir/(modelName+\"_quant.tflite\")\n",
        "  tflite_model_quant_file.write_bytes(tflite_model_quant)"
      ],
      "metadata": {
        "id": "ybB6PM3i_2hg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can verify that all models have been correctly saved by printing the contents of the /root/tflite_models directory. You should be able to see two files representing each of the models, one for the original and one for the quantized version. Note that the size of the original models is significantly bigger than the size of their quantized version."
      ],
      "metadata": {
        "id": "a4R9lhIksuB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /root/tflite_models"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mYWalwzotPLo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}