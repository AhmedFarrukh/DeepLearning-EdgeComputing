{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedFarrukh/DeepLearning-EdgeComputing/blob/main/notebooks/CPU_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAmCHY3Svxm8"
      },
      "source": [
        "In this notebook, we test the inference times of quantized models, and their original versions, on a CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JysRuQcv4YI"
      },
      "source": [
        "## **Prepare the Resource**\n",
        "This notebook will try to reserve the compute_cascadelake_r device available on CHI@UC.\n",
        "\n",
        "### **Check Availability**\n",
        "Before you begin, you should check the host calendar at https://chi.uc.chameleoncloud.org/project/leases/calendar/host/ to see what node types are available.\n",
        "\n",
        "### **Chameleon Configuration**\n",
        "You can change your Chameleon project name (if not using the one that is automatically configured in the JupyterHub environment) and the site on which to reserve resources (depending on availability) in the following cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s7hjrPcyICs"
      },
      "source": [
        "If you need to change the details of the Chameleon server, e.g. use a different edge device (NODE_TYPE), or a different node type depending on availability, you can also do that in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrbUlbRVvrml",
        "outputId": "6451ccbf-0e0a-4a79-ef48-a460c5cb46a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now using CHI@UC:\n",
            "URL: https://chi.uc.chameleoncloud.org\n",
            "Location: Argonne National Laboratory, Lemont, Illinois, USA\n",
            "Support contact: help@chameleoncloud.org\n"
          ]
        }
      ],
      "source": [
        "import chi, os, time\n",
        "from chi import lease\n",
        "from chi import server\n",
        "\n",
        "PROJECT_NAME = os.getenv('OS_PROJECT_NAME') # change this if you need to\n",
        "chi.use_site(\"CHI@UC\")\n",
        "chi.set(\"project_name\", PROJECT_NAME)\n",
        "username = os.getenv('USER') # all exp resources will have this prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4TlrIWdyLR1"
      },
      "outputs": [],
      "source": [
        "chi.set(\"image\", \"CC-Ubuntu20.04\")\n",
        "NODE_TYPE = \"compute_cascadelake_r\"\n",
        "expname = \"cpu-inference\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHS6pLZWyc9_"
      },
      "outputs": [],
      "source": [
        "res = []\n",
        "lease.add_node_reservation(res, node_type=NODE_TYPE, count=1)\n",
        "lease.add_fip_reservation(res, count=1)\n",
        "start_date, end_date = lease.lease_duration(days=0, hours=8)\n",
        "\n",
        "l = lease.create_lease(f\"{username}-{NODE_TYPE}\", res, start_date=start_date, end_date=end_date)\n",
        "l = lease.wait_for_active(l[\"id\"])  #Comment this line if the lease starts in the future"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnU8vOUuyh2g",
        "outputId": "77004118-5810-4467-e33e-cfa72def6597"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'f950ba18-ba4a-4849-8193-c70e61aa9452'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# continue here, whether using a lease created just now or one created earlier\n",
        "l = lease.get_lease(f\"{username}-{NODE_TYPE}\")\n",
        "l['id']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg2azf9KeCGS"
      },
      "source": [
        "### **Provisioning Resources**\n",
        "This cell provisions resources. It will take approximately 15 minutes. You can check on its status in the Chameleon web-based UI: https://chi.uc.chameleoncloud.org/project/instances/, then come back here when it is in the READY state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-CLk8JveO3X",
        "outputId": "becbf701-902e-4e2b-a4cb-99c051714b76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "openstack.compute.v2.server.Server(id=80569ed3-f37c-481d-8838-4fac8389cfce, name=ahmed_farrukh_nyu_edu-compute_cascadelake_r, status=ACTIVE, tenant_id=cb970a4b0f2e42c9b1b3f9015d02f8a5, user_id=042ab1e0e3f7c495647249cc7d377c5e9031a04fccce623cea2f51f120a9bd5a, metadata={}, hostId=c9f7f1389ac96a530d6c179791ae187dfcdcf6acd066d7d067c56aa6, image={'id': '2be02db9-e591-47b4-9dd3-1d23f7a01433', 'links': [{'rel': 'bookmark', 'href': 'https://chi.uc.chameleoncloud.org:8774/images/2be02db9-e591-47b4-9dd3-1d23f7a01433'}]}, flavor={'vcpus': 1, 'ram': 1, 'disk': 20, 'ephemeral': 0, 'swap': 0, 'original_name': 'baremetal', 'extra_specs': {'resources:CUSTOM_BAREMETAL': '1', 'resources:VCPU': '0', 'resources:MEMORY_MB': '0', 'resources:DISK_GB': '0'}}, created=2024-07-18T15:51:17Z, updated=2024-07-18T16:06:24Z, addresses={'sharednet1': [{'version': 4, 'addr': '10.140.83.253', 'OS-EXT-IPS:type': 'fixed', 'OS-EXT-IPS-MAC:mac_addr': 'b8:ce:f6:43:4f:97'}]}, accessIPv4=, accessIPv6=, links=[{'rel': 'self', 'href': 'https://chi.uc.chameleoncloud.org:8774/v2.1/servers/80569ed3-f37c-481d-8838-4fac8389cfce'}, {'rel': 'bookmark', 'href': 'https://chi.uc.chameleoncloud.org:8774/servers/80569ed3-f37c-481d-8838-4fac8389cfce'}], OS-DCF:diskConfig=MANUAL, progress=0, OS-EXT-AZ:availability_zone=nova, config_drive=True, key_name=ahmed_farrukh_nyu_edu-jupyter, OS-SRV-USG:launched_at=2024-07-18T16:06:24.000000, OS-SRV-USG:terminated_at=None, OS-EXT-SRV-ATTR:host=mgmt01-ironic, OS-EXT-SRV-ATTR:instance_name=instance-0001283d, OS-EXT-SRV-ATTR:hypervisor_hostname=e0477777-88fe-42ec-a686-87e646e02e81, OS-EXT-SRV-ATTR:reservation_id=r-eayrt1ki, OS-EXT-SRV-ATTR:launch_index=0, OS-EXT-SRV-ATTR:hostname=ahmed-farrukh-nyu-edu-compute-cascadelake-r, OS-EXT-SRV-ATTR:kernel_id=, OS-EXT-SRV-ATTR:ramdisk_id=, OS-EXT-SRV-ATTR:root_device_name=/dev/sda, OS-EXT-SRV-ATTR:user_data=None, OS-EXT-STS:task_state=None, OS-EXT-STS:vm_state=active, OS-EXT-STS:power_state=1, os-extended-volumes:volumes_attached=[], locked=False, description=ahmed_farrukh_nyu_edu-compute_cascadelake_r, tags=[], trusted_image_certificates=None, server_groups=[], security_groups=[{'name': 'default'}], location=Munch({'cloud': 'chi.uc.chameleoncloud.org', 'region_name': None, 'zone': 'nova', 'project': Munch({'id': 'cb970a4b0f2e42c9b1b3f9015d02f8a5', 'name': None, 'domain_id': None, 'domain_name': None})}))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reservation_id = lease.get_node_reservation(l[\"id\"])\n",
        "server.create_server(\n",
        "    f\"{username}-{NODE_TYPE}\",\n",
        "    reservation_id=reservation_id,\n",
        "    image_name=chi.get(\"image\")\n",
        ")\n",
        "server_id = server.get_server_id(f\"{username}-{NODE_TYPE}\")\n",
        "server.wait_for_active(server_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu5Hzr9FehN8"
      },
      "source": [
        "\n",
        "Associate an IP address with this server:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZM0NH-negzu",
        "outputId": "2ffa7a0f-7f51-4902-cc29-2ef5f4a179cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'192.5.87.186'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reserved_fip = lease.get_reserved_floating_ips(l[\"id\"])[0]\n",
        "server.associate_floating_ip(server_id,reserved_fip)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSMjuUhcek4U",
        "tags": []
      },
      "source": [
        "\n",
        "and wait for it to come up:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsHyzLuKejeT"
      },
      "outputs": [],
      "source": [
        "server.wait_for_tcp(reserved_fip, port=22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FJBvXw_epHx"
      },
      "source": [
        "### **Install Basic Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O4CXHF9ex5y"
      },
      "outputs": [],
      "source": [
        "from chi import ssh\n",
        "node = ssh.Remote(reserved_fip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZehBVfJez5E",
        "outputId": "8173d781-e740-459b-906c-2330e45700e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/paramiko/client.py:852: UserWarning: Unknown ssh-ed25519 host key for 192.5.87.186: b'14d282030a585e40b2a99ef68477d62e'\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hit:1 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-backports InRelease [128 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3426 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 c-n-f Metadata [17.7 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3074 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 c-n-f Metadata [540 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1210 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 c-n-f Metadata [27.5 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [27.1 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 c-n-f Metadata [616 B]\n",
            "Get:13 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3056 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu focal-security/main amd64 c-n-f Metadata [14.0 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2956 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 c-n-f Metadata [544 B]\n",
            "Get:17 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [991 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu focal-security/universe amd64 c-n-f Metadata [20.9 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [24.8 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 c-n-f Metadata [540 B]\n",
            "Fetched 15.2 MB in 2s (7082 kB/s)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "77 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "python3-dev is already the newest version (3.8.2-0ubuntu2).\n",
            "The following NEW packages will be installed:\n",
            "  python3-pip python3-wheel\n",
            "0 upgraded, 2 newly installed, 0 to remove and 77 not upgraded.\n",
            "Need to get 254 kB of archives.\n",
            "After this operation, 1154 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-wheel all 0.34.2-1ubuntu0.1 [23.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-pip all 20.0.2-5ubuntu1.10 [231 kB]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetched 254 kB in 0s (1432 kB/s)\n",
            "Selecting previously unselected package python3-wheel.\n",
            "(Reading database ... 82169 files and directories currently installed.)\n",
            "Preparing to unpack .../python3-wheel_0.34.2-1ubuntu0.1_all.deb ...\n",
            "Unpacking python3-wheel (0.34.2-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../python3-pip_20.0.2-5ubuntu1.10_all.deb ...\n",
            "Unpacking python3-pip (20.0.2-5ubuntu1.10) ...\n",
            "Setting up python3-wheel (0.34.2-1ubuntu0.1) ...\n",
            "Setting up python3-pip (20.0.2-5ubuntu1.10) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Collecting pip\n",
            "  Downloading pip-24.1.2-py3-none-any.whl (1.8 MB)\n",
            "Installing collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 20.0.2\n",
            "    Not uninstalling pip at /usr/lib/python3/dist-packages, outside environment /usr\n",
            "    Can't uninstall 'pip'. No files were found to uninstall.\n",
            "Successfully installed pip-24.1.2\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Result cmd='sudo pip3 install --upgrade pip' exited=0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "node.run('sudo apt update')\n",
        "node.run('sudo apt -y install python3-pip python3-dev')\n",
        "node.run('sudo pip3 install --upgrade pip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58wuXJaFe8KS"
      },
      "source": [
        "#### **Install Python Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZSgadcvfAEQ",
        "outputId": "456d7b0f-2a9a-451f-e5f2-ba03a4388086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow)\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=23.1.21 (from tensorflow)\n",
            "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
            "  Downloading grpcio-1.65.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting h5py>=2.9.0 (from tensorflow)\n",
            "  Downloading h5py-3.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting numpy<=1.24.3,>=1.22 (from tensorflow)\n",
            "  Downloading numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting packaging (from tensorflow)\n",
            "  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (45.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow)\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow)\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow)\n",
            "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting wrapt>=1.11.0 (from tensorflow)\n",
            "  Downloading wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.34.2)\n",
            "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow)\n",
            "  Downloading google_auth-2.32.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow)\n",
            "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.22.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard<2.14,>=2.13->tensorflow)\n",
            "  Downloading werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow)\n",
            "  Downloading cachetools-5.4.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.2.1)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow)\n",
            "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow)\n",
            "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting importlib-metadata>=4.4 (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow)\n",
            "  Downloading importlib_metadata-8.0.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow)\n",
            "  Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.4.2)\n",
            "Downloading tensorflow-2.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 479.6/479.6 MB 6.9 MB/s eta 0:00:00\n",
            "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 29.7 MB/s eta 0:00:00\n",
            "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
            "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 11.2 MB/s eta 0:00:00\n",
            "Downloading grpcio-1.65.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.7/5.7 MB 59.8 MB/s eta 0:00:00\n",
            "Downloading h5py-3.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/5.3 MB 62.2 MB/s eta 0:00:00\n",
            "Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 77.8 MB/s eta 0:00:00\n",
            "Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.5/24.5 MB 46.6 MB/s eta 0:00:00\n",
            "Downloading numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 44.2 MB/s eta 0:00:00\n",
            "Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 17.2 MB/s eta 0:00:00\n",
            "Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.6/294.6 kB 40.3 MB/s eta 0:00:00\n",
            "Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 44.9 MB/s eta 0:00:00\n",
            "Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 440.8/440.8 kB 51.8 MB/s eta 0:00:00\n",
            "Downloading tensorflow_io_gcs_filesystem-0.34.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 49.1 MB/s eta 0:00:00\n",
            "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Downloading wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.4/83.4 kB 21.4 MB/s eta 0:00:00\n",
            "Downloading packaging-24.1-py3-none-any.whl (53 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 kB 2.8 MB/s eta 0:00:00\n",
            "Downloading google_auth-2.32.0-py2.py3-none-any.whl (195 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195.5/195.5 kB 34.3 MB/s eta 0:00:00\n",
            "Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.4/105.4 kB 22.7 MB/s eta 0:00:00\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 47.0 MB/s eta 0:00:00\n",
            "Downloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.3/227.3 kB 37.3 MB/s eta 0:00:00\n",
            "Downloading cachetools-5.4.0-py3-none-any.whl (9.5 kB)\n",
            "Downloading importlib_metadata-8.0.0-py3-none-any.whl (24 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26 kB)\n",
            "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Installing collected packages: libclang, flatbuffers, wrapt, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, requests-oauthlib, protobuf, packaging, numpy, MarkupSafe, keras, importlib-metadata, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, werkzeug, opt-einsum, markdown, h5py, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/home/cc/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The scripts f2py, f2py3 and f2py3.8 are installed in '/home/cc/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script markdown_py is installed in '/home/cc/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script google-oauthlib-tool is installed in '/home/cc/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script tensorboard is installed in '/home/cc/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/cc/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully installed MarkupSafe-2.1.5 absl-py-2.1.0 astunparse-1.6.3 cachetools-5.4.0 flatbuffers-24.3.25 gast-0.4.0 google-auth-2.32.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.65.1 h5py-3.11.0 importlib-metadata-8.0.0 keras-2.13.1 libclang-18.1.1 markdown-3.6 numpy-1.24.3 opt-einsum-3.3.0 packaging-24.1 protobuf-4.25.3 requests-oauthlib-2.0.0 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.2 tensorflow-2.13.1 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.4.0 typing-extensions-4.5.0 werkzeug-3.0.3 wrapt-1.16.0\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.7 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Downloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Downloading fonttools-4.53.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (162 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 162.6/162.6 kB 6.0 MB/s eta 0:00:00\n",
            "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
            "  Downloading kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.20 in ./.local/lib/python3.8/site-packages (from matplotlib) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.8/site-packages (from matplotlib) (24.1)\n",
            "Collecting pillow>=6.2.0 (from matplotlib)\n",
            "  Downloading pillow-10.4.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
            "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting python-dateutil>=2.7 (from matplotlib)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
            "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting zipp>=3.1.0 (from importlib-resources>=3.2.0->matplotlib)\n",
            "  Downloading zipp-3.19.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.14.0)\n",
            "Downloading matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.2/9.2 MB 98.7 MB/s eta 0:00:00\n",
            "Downloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.1/301.1 kB 51.1 MB/s eta 0:00:00\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.53.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 129.3 MB/s eta 0:00:00\n",
            "Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
            "Downloading kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 77.6 MB/s eta 0:00:00\n",
            "Downloading pillow-10.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 110.9 MB/s eta 0:00:00\n",
            "Downloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.2/103.2 kB 29.7 MB/s eta 0:00:00\n",
            "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.9/229.9 kB 48.7 MB/s eta 0:00:00\n",
            "Downloading zipp-3.19.2-py3-none-any.whl (9.0 kB)\n",
            "Installing collected packages: zipp, python-dateutil, pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, importlib-resources, matplotlib\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The scripts fonttools, pyftmerge, pyftsubset and ttx are installed in '/home/cc/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully installed contourpy-1.1.1 cycler-0.12.1 fonttools-4.53.1 importlib-resources-6.4.0 kiwisolver-1.4.5 matplotlib-3.7.5 pillow-10.4.0 pyparsing-3.1.2 python-dateutil-2.9.0.post0 zipp-3.19.2\n",
            "Collecting pathlib\n",
            "  Downloading pathlib-1.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading pathlib-1.0.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pathlib\n",
            "Successfully installed pathlib-1.0.1\n",
            "Requirement already satisfied: numpy in ./.local/lib/python3.8/site-packages (1.24.3)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Result cmd='python3 -m pip install --user numpy' exited=0>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "node.run('python3 -m pip install --user tensorflow')\n",
        "node.run('python3 -m pip install --user matplotlib')\n",
        "node.run('python3 -m pip install --user pathlib')\n",
        "node.run('python3 -m pip install --user numpy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgTNS0t7fBQM"
      },
      "source": [
        "### **Retrieve Materials**\n",
        "Finally, get a copy of the code you will run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8EdMWzqfLb7",
        "outputId": "974ff3a7-8f7f-45da-9466-07892e14f99f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'experimental'...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Result cmd='git clone https://github.com/AhmedFarrukh/experimental.git' exited=0>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "node.run('git clone https://github.com/AhmedFarrukh/experimental.git')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xo-6M4j5HXl"
      },
      "source": [
        "### **Run Experiment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juxrHBNS5HXl"
      },
      "source": [
        "Verify that the code files have correctly been loaded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "6Jbl37HQ5HXl",
        "outputId": "2a7504af-d86a-4680-b405-3f05e23cc7e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "measuringInferenceTimes.py\n",
            "quantizingModels.py\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Result cmd='ls ./experimental' exited=0>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "node.run('ls ./experimental')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiE5CSKf5HXl"
      },
      "source": [
        "Run the following cell to load CNN models and apply Dynamic Range Quantization. Both original and quantized models are saved in the ./tflite_models directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_e0QsyF5HXl",
        "outputId": "c80ee788-0d0f-4a90-d679-47ab713d5050"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-18 16:18:05.795386: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-18 16:18:05.797017: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-07-18 16:18:05.830043: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-07-18 16:18:05.830476: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-18 16:18:06.489152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf.h5\n",
            "17225924/17225924 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-18 16:18:14.871447: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
            "2024-07-18 16:18:14.871473: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
            "2024-07-18 16:18:14.872076: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpeld_ljls\n",
            "2024-07-18 16:18:14.884092: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
            "2024-07-18 16:18:14.884110: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpeld_ljls\n",
            "2024-07-18 16:18:14.913755: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
            "2024-07-18 16:18:14.920899: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
            "2024-07-18 16:18:15.146122: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpeld_ljls\n",
            "2024-07-18 16:18:15.212238: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 340164 microseconds.\n",
            "2024-07-18 16:18:15.293626: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2024-07-18 16:18:22.589504: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
            "2024-07-18 16:18:22.589548: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
            "2024-07-18 16:18:22.589741: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpgweuc5_w\n",
            "2024-07-18 16:18:22.601784: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
            "2024-07-18 16:18:22.601803: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpgweuc5_w\n",
            "2024-07-18 16:18:22.634750: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
            "2024-07-18 16:18:22.831291: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpgweuc5_w\n",
            "2024-07-18 16:18:22.900597: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 310857 microseconds.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "96112376/96112376 [==============================] - 2s 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-18 16:18:49.378236: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
            "2024-07-18 16:18:49.378266: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
            "2024-07-18 16:18:49.378479: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpg7evy3yd\n",
            "2024-07-18 16:18:49.415427: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
            "2024-07-18 16:18:49.415452: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpg7evy3yd\n",
            "2024-07-18 16:18:49.523403: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
            "2024-07-18 16:18:50.130132: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpg7evy3yd\n",
            "2024-07-18 16:18:50.344587: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 966109 microseconds.\n",
            "2024-07-18 16:19:14.938246: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
            "2024-07-18 16:19:14.938276: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
            "2024-07-18 16:19:14.938479: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpq2bi98w0\n",
            "2024-07-18 16:19:14.974692: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
            "2024-07-18 16:19:14.974717: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpq2bi98w0\n",
            "2024-07-18 16:19:15.084120: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
            "2024-07-18 16:19:15.683406: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpq2bi98w0\n",
            "2024-07-18 16:19:15.898827: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 960349 microseconds.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
            "102967424/102967424 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-18 16:19:38.051910: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
            "2024-07-18 16:19:38.051939: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
            "2024-07-18 16:19:38.052108: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpklrvurp8\n",
            "2024-07-18 16:19:38.075769: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
            "2024-07-18 16:19:38.075790: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpklrvurp8\n",
            "2024-07-18 16:19:38.149117: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
            "2024-07-18 16:19:38.644083: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpklrvurp8\n",
            "2024-07-18 16:19:38.803535: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 751429 microseconds.\n",
            "2024-07-18 16:19:55.439888: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
            "2024-07-18 16:19:55.439920: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
            "2024-07-18 16:19:55.440132: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpmsg4pqg_\n",
            "2024-07-18 16:19:55.464948: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
            "2024-07-18 16:19:55.464970: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpmsg4pqg_\n",
            "2024-07-18 16:19:55.543510: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
            "2024-07-18 16:19:56.017083: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpmsg4pqg_\n",
            "2024-07-18 16:19:56.175953: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 735820 microseconds.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels.h5\n",
            "179648224/179648224 [==============================] - 2s 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-18 16:20:33.796796: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
            "2024-07-18 16:20:33.796828: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
            "2024-07-18 16:20:33.797008: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpiodk16g6\n",
            "2024-07-18 16:20:33.844591: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
            "2024-07-18 16:20:33.844620: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpiodk16g6\n",
            "2024-07-18 16:20:33.996093: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
            "2024-07-18 16:20:35.005411: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpiodk16g6\n",
            "2024-07-18 16:20:35.328113: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 1531105 microseconds.\n",
            "2024-07-18 16:21:08.178221: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
            "2024-07-18 16:21:08.178251: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
            "2024-07-18 16:21:08.178472: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp_jwncw58\n",
            "2024-07-18 16:21:08.225587: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
            "2024-07-18 16:21:08.225615: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmp_jwncw58\n",
            "2024-07-18 16:21:08.376528: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
            "2024-07-18 16:21:09.365739: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmp_jwncw58\n",
            "2024-07-18 16:21:09.694676: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 1516205 microseconds.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152_weights_tf_dim_ordering_tf_kernels.h5\n",
            "242900224/242900224 [==============================] - 3s 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-18 16:22:06.834698: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
            "2024-07-18 16:22:06.834732: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
            "2024-07-18 16:22:06.834935: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp8cikreot\n",
            "2024-07-18 16:22:06.904837: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
            "2024-07-18 16:22:06.904869: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmp8cikreot\n",
            "2024-07-18 16:22:07.134635: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
            "2024-07-18 16:22:08.645192: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmp8cikreot\n",
            "2024-07-18 16:22:09.133498: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 2298562 microseconds.\n",
            "2024-07-18 16:22:57.098668: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
            "2024-07-18 16:22:57.098703: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
            "2024-07-18 16:22:57.098896: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpok2slb32\n",
            "2024-07-18 16:22:57.170984: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
            "2024-07-18 16:22:57.171018: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpok2slb32\n",
            "2024-07-18 16:22:57.397773: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
            "2024-07-18 16:22:58.796418: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpok2slb32\n",
            "2024-07-18 16:22:59.252925: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 2154029 microseconds.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467096/553467096 [==============================] - 5s 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-18 16:23:20.517173: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
            "2024-07-18 16:23:20.517201: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
            "2024-07-18 16:23:20.517364: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpzpek89g3\n",
            "2024-07-18 16:23:20.522589: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
            "2024-07-18 16:23:20.522607: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpzpek89g3\n",
            "2024-07-18 16:23:20.534868: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
            "2024-07-18 16:23:20.871684: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpzpek89g3\n",
            "2024-07-18 16:23:20.901440: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 384076 microseconds.\n",
            "2024-07-18 16:25:23.181804: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
            "2024-07-18 16:25:23.181829: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
            "2024-07-18 16:25:23.181992: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp4p8hk1h5\n",
            "2024-07-18 16:25:23.184842: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
            "2024-07-18 16:25:23.184858: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmp4p8hk1h5\n",
            "2024-07-18 16:25:23.192026: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
            "2024-07-18 16:25:23.463509: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmp4p8hk1h5\n",
            "2024-07-18 16:25:23.488024: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 306032 microseconds.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels.h5\n",
            "574710816/574710816 [==============================] - 8s 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-18 16:27:42.329317: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
            "2024-07-18 16:27:42.329345: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
            "2024-07-18 16:27:42.329509: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp1er0hrru\n",
            "2024-07-18 16:27:42.332804: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
            "2024-07-18 16:27:42.332821: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmp1er0hrru\n",
            "2024-07-18 16:27:42.341380: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
            "2024-07-18 16:27:42.638217: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmp1er0hrru\n",
            "2024-07-18 16:27:42.666694: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 337186 microseconds.\n",
            "2024-07-18 16:29:42.329033: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
            "2024-07-18 16:29:42.329059: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
            "2024-07-18 16:29:42.329240: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpyyzkemln\n",
            "2024-07-18 16:29:42.335128: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
            "2024-07-18 16:29:42.335147: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpyyzkemln\n",
            "2024-07-18 16:29:42.349287: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
            "2024-07-18 16:29:42.639632: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpyyzkemln\n",
            "2024-07-18 16:29:42.674058: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 344818 microseconds.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Result cmd='python3 ./experimental/quantizingModels.py' exited=0>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "node.run('python3 ./experimental/quantizingModels.py')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHZKLDQO5HXm"
      },
      "source": [
        "Run the next cell to load the benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Wz5lrLLn5HXm",
        "outputId": "72b39897-e1a8-4350-8bbd-dc71284006cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "--2024-07-18 16:31:58--  https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/linux_x86-64_benchmark_model\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.40.155, 142.251.40.187, 142.251.40.219, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.40.155|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6237672 (5.9M) [application/octet-stream]\n",
            "Saving to: ‘./benchmark/linux_x86-64_benchmark_model’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  0% 1.12M 5s\n",
            "    50K .......... .......... .......... .......... ..........  1% 2.53M 4s\n",
            "   100K .......... .......... .......... .......... ..........  2% 3.89M 3s\n",
            "   150K .......... .......... .......... .......... ..........  3% 5.44M 2s\n",
            "   200K .......... .......... .......... .......... ..........  4% 6.25M 2s\n",
            "   250K .......... .......... .......... .......... ..........  4% 8.05M 2s\n",
            "   300K .......... .......... .......... .......... ..........  5% 10.4M 2s\n",
            "   350K .......... .......... .......... .......... ..........  6% 11.5M 2s\n",
            "   400K .......... .......... .......... .......... ..........  7% 12.3M 1s\n",
            "   450K .......... .......... .......... .......... ..........  8% 15.2M 1s\n",
            "   500K .......... .......... .......... .......... ..........  9% 17.3M 1s\n",
            "   550K .......... .......... .......... .......... ..........  9% 17.5M 1s\n",
            "   600K .......... .......... .......... .......... .......... 10% 18.4M 1s\n",
            "   650K .......... .......... .......... .......... .......... 11% 20.8M 1s\n",
            "   700K .......... .......... .......... .......... .......... 12% 17.8M 1s\n",
            "   750K .......... .......... .......... .......... .......... 13% 25.8M 1s\n",
            "   800K .......... .......... .......... .......... .......... 13% 19.9M 1s\n",
            "   850K .......... .......... .......... .......... .......... 14% 38.8M 1s\n",
            "   900K .......... .......... .......... .......... .......... 15% 25.4M 1s\n",
            "   950K .......... .......... .......... .......... .......... 16% 35.1M 1s\n",
            "  1000K .......... .......... .......... .......... .......... 17% 29.6M 1s\n",
            "  1050K .......... .......... .......... .......... .......... 18% 26.7M 1s\n",
            "  1100K .......... .......... .......... .......... .......... 18% 41.2M 1s\n",
            "  1150K .......... .......... .......... .......... .......... 19% 32.0M 1s\n",
            "  1200K .......... .......... .......... .......... .......... 20% 54.6M 1s\n",
            "  1250K .......... .......... .......... .......... .......... 21% 29.6M 1s\n",
            "  1300K .......... .......... .......... .......... .......... 22% 36.2M 1s\n",
            "  1350K .......... .......... .......... .......... .......... 22% 46.8M 0s\n",
            "  1400K .......... .......... .......... .......... .......... 23% 39.2M 0s\n",
            "  1450K .......... .......... .......... .......... .......... 24% 59.8M 0s\n",
            "  1500K .......... .......... .......... .......... .......... 25% 43.2M 0s\n",
            "  1550K .......... .......... .......... .......... .......... 26% 33.8M 0s\n",
            "  1600K .......... .......... .......... .......... .......... 27% 70.9M 0s\n",
            "  1650K .......... .......... .......... .......... .......... 27% 37.9M 0s\n",
            "  1700K .......... .......... .......... .......... .......... 28% 61.5M 0s\n",
            "  1750K .......... .......... .......... .......... .......... 29% 48.5M 0s\n",
            "  1800K .......... .......... .......... .......... .......... 30% 45.3M 0s\n",
            "  1850K .......... .......... .......... .......... .......... 31% 84.5M 0s\n",
            "  1900K .......... .......... .......... .......... .......... 32% 78.8M 0s\n",
            "  1950K .......... .......... .......... .......... .......... 32% 58.6M 0s\n",
            "  2000K .......... .......... .......... .......... .......... 33% 51.3M 0s\n",
            "  2050K .......... .......... .......... .......... .......... 34% 52.2M 0s\n",
            "  2100K .......... .......... .......... .......... .......... 35% 60.6M 0s\n",
            "  2150K .......... .......... .......... .......... .......... 36% 86.7M 0s\n",
            "  2200K .......... .......... .......... .......... .......... 36% 70.5M 0s\n",
            "  2250K .......... .......... .......... .......... .......... 37% 72.2M 0s\n",
            "  2300K .......... .......... .......... .......... .......... 38% 62.0M 0s\n",
            "  2350K .......... .......... .......... .......... .......... 39% 71.5M 0s\n",
            "  2400K .......... .......... .......... .......... .......... 40%  130M 0s\n",
            "  2450K .......... .......... .......... .......... .......... 41% 57.6M 0s\n",
            "  2500K .......... .......... .......... .......... .......... 41% 62.7M 0s\n",
            "  2550K .......... .......... .......... .......... .......... 42% 90.9M 0s\n",
            "  2600K .......... .......... .......... .......... .......... 43% 70.1M 0s\n",
            "  2650K .......... .......... .......... .......... .......... 44% 99.6M 0s\n",
            "  2700K .......... .......... .......... .......... .......... 45%  102M 0s\n",
            "  2750K .......... .......... .......... .......... .......... 45% 78.4M 0s\n",
            "  2800K .......... .......... .......... .......... .......... 46% 75.1M 0s\n",
            "  2850K .......... .......... .......... .......... .......... 47% 72.2M 0s\n",
            "  2900K .......... .......... .......... .......... .......... 48%  146M 0s\n",
            "  2950K .......... .......... .......... .......... .......... 49% 91.9M 0s\n",
            "  3000K .......... .......... .......... .......... .......... 50% 82.8M 0s\n",
            "  3050K .......... .......... .......... .......... .......... 50% 78.5M 0s\n",
            "  3100K .......... .......... .......... .......... .......... 51% 60.6M 0s\n",
            "  3150K .......... .......... .......... .......... .......... 52%  100M 0s\n",
            "  3200K .......... .......... .......... .......... .......... 53%  146M 0s\n",
            "  3250K .......... .......... .......... .......... .......... 54% 51.2M 0s\n",
            "  3300K .......... .......... .......... .......... .......... 54%  145M 0s\n",
            "  3350K .......... .......... .......... .......... .......... 55%  112M 0s\n",
            "  3400K .......... .......... .......... .......... .......... 56%  131M 0s\n",
            "  3450K .......... .......... .......... .......... .......... 57%  126M 0s\n",
            "  3500K .......... .......... .......... .......... .......... 58% 91.6M 0s\n",
            "  3550K .......... .......... .......... .......... .......... 59%  110M 0s\n",
            "  3600K .......... .......... .......... .......... .......... 59%  103M 0s\n",
            "  3650K .......... .......... .......... .......... .......... 60%  122M 0s\n",
            "  3700K .......... .......... .......... .......... .......... 61%  136M 0s\n",
            "  3750K .......... .......... .......... .......... .......... 62%  113M 0s\n",
            "  3800K .......... .......... .......... .......... .......... 63%  102M 0s\n",
            "  3850K .......... .......... .......... .......... .......... 64%  102M 0s\n",
            "  3900K .......... .......... .......... .......... .......... 64%  145M 0s\n",
            "  3950K .......... .......... .......... .......... .......... 65%  138M 0s\n",
            "  4000K .......... .......... .......... .......... .......... 66%  121M 0s\n",
            "  4050K .......... .......... .......... .......... .......... 67%  123M 0s\n",
            "  4100K .......... .......... .......... .......... .......... 68% 99.9M 0s\n",
            "  4150K .......... .......... .......... .......... .......... 68%  161M 0s\n",
            "  4200K .......... .......... .......... .......... .......... 69%  136M 0s\n",
            "  4250K .......... .......... .......... .......... .......... 70%  116M 0s\n",
            "  4300K .......... .......... .......... .......... .......... 71%  141M 0s\n",
            "  4350K .......... .......... .......... .......... .......... 72%  125M 0s\n",
            "  4400K .......... .......... .......... .......... .......... 73%  120M 0s\n",
            "  4450K .......... .......... .......... .......... .......... 73%  181M 0s\n",
            "  4500K .......... .......... .......... .......... .......... 74%  128M 0s\n",
            "  4550K .......... .......... .......... .......... .......... 75%  145M 0s\n",
            "  4600K .......... .......... .......... .......... .......... 76%  126M 0s\n",
            "  4650K .......... .......... .......... .......... .......... 77%  140M 0s\n",
            "  4700K .......... .......... .......... .......... .......... 77%  168M 0s\n",
            "  4750K .......... .......... .......... .......... .......... 78%  148M 0s\n",
            "  4800K .......... .......... .......... .......... .......... 79%  129M 0s\n",
            "  4850K .......... .......... .......... .......... .......... 80%  148M 0s\n",
            "  4900K .......... .......... .......... .......... .......... 81%  159M 0s\n",
            "  4950K .......... .......... .......... .......... .......... 82%  168M 0s\n",
            "  5000K .......... .......... .......... .......... .......... 82%  137M 0s\n",
            "  5050K .......... .......... .......... .......... .......... 83%  146M 0s\n",
            "  5100K .......... .......... .......... .......... .......... 84%  166M 0s\n",
            "  5150K .......... .......... .......... .......... .......... 85%  188M 0s\n",
            "  5200K .......... .......... .......... .......... .......... 86% 6.44M 0s\n",
            "  5250K .......... .......... .......... .......... .......... 87%  237M 0s\n",
            "  5300K .......... .......... .......... .......... .......... 87%  190M 0s\n",
            "  5350K .......... .......... .......... .......... .......... 88%  389M 0s\n",
            "  5400K .......... .......... .......... .......... .......... 89%  219M 0s\n",
            "  5450K .......... .......... .......... .......... .......... 90%  232M 0s\n",
            "  5500K .......... .......... .......... .......... .......... 91%  143M 0s\n",
            "  5550K .......... .......... .......... .......... .......... 91%  190M 0s\n",
            "  5600K .......... .......... .......... .......... .......... 92%  160M 0s\n",
            "  5650K .......... .......... .......... .......... .......... 93%  221M 0s\n",
            "  5700K .......... .......... .......... .......... .......... 94%  312M 0s\n",
            "  5750K .......... .......... .......... .......... .......... 95%  186M 0s\n",
            "  5800K .......... .......... .......... .......... .......... 96%  203M 0s\n",
            "  5850K .......... .......... .......... .......... .......... 96%  228M 0s\n",
            "  5900K .......... .......... .......... .......... .......... 97%  270M 0s\n",
            "  5950K .......... .......... .......... .......... .......... 98%  204M 0s\n",
            "  6000K .......... .......... .......... .......... .......... 99%  217M 0s\n",
            "  6050K .......... .......... .......... .......... .         100%  260M=0.2s\n",
            "\n",
            "2024-07-18 16:31:59 (29.5 MB/s) - ‘./benchmark/linux_x86-64_benchmark_model’ saved [6237672/6237672]\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Result cmd='chmod +x ./benchmark/linux_x86-64_benchmark_model' exited=0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "node.run('mkdir ./benchmark')\n",
        "node.run('wget https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/linux_x86-64_benchmark_model -P ./benchmark')\n",
        "node.run('chmod +x ./benchmark/linux_x86-64_benchmark_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_BwqVTy5HXm"
      },
      "source": [
        "Finally, use the benchmark to measure the inference time and memory footprint of each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hL3BH5yU5HXm",
        "outputId": "3f4215f9-3494-4a10-8b8c-ac5dea58c35a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-18 16:32:17.106038: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-18 16:32:17.107730: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-07-18 16:32:17.141476: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-07-18 16:32:17.141900: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-18 16:32:17.789760: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MobileNet\n",
            "InceptionV3\n",
            "ResNet50\n",
            "ResNet101\n",
            "ResNet152\n",
            "VGG16\n",
            "VGG19\n",
            "MobileNet_quant\n",
            "Error with model:  MobileNet_quant\n",
            "{'MobileNet_quant': {'Init Time (ms)': 42.392, 'Inference Timings (us)': {'Init': 42392, 'First Inference': 9140, 'Warmup (avg)': 8202.95, 'Inference (avg)': 8184.14}}}\n",
            "INFO: STARTING!\n",
            "INFO: Log parameter values verbosely: [0]\n",
            "INFO: Num threads: [1]\n",
            "INFO: Graph: [./tflite_models/MobileNet_quant.tflite]\n",
            "INFO: Signature to run: []\n",
            "INFO: #threads used for CPU inference: [1]\n",
            "INFO: Loaded model ./tflite_models/MobileNet_quant.tflite\n",
            "INFO: The input model file size (MB): 4.42374\n",
            "INFO: Initialized session in 42.392ms.\n",
            "INFO: Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\n",
            "INFO: count=61 first=9140 curr=8263 min=8155 max=9140 avg=8202.95 std=124\n",
            "\n",
            "INFO: Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\n",
            "INFO: count=122 first=8170 curr=8190 min=8147 max=8280 avg=8184.14 std=21\n",
            "\n",
            "INFO: Inference timings in us: Init: 42392, First inference: 9140, Warmup (avg): 8202.95, Inference (avg): 8184.14\n",
            "INFO: Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\n",
            "INFO: Memory footprint delta from the start of the tool (MB): init=18 overall=23.1328\n",
            "\n",
            "InceptionV3_quant\n",
            "ResNet50_quant\n",
            "ResNet101_quant\n",
            "ResNet152_quant\n",
            "VGG16_quant\n",
            "VGG19_quant\n",
            "MobileNet\n",
            "Init Time :  (45.03015, 2.568072188586115)\n",
            "Init Inference :  (45030.15, 2568.072188586114)\n",
            "First Inference :  (11469.15, 141.45457705009426)\n",
            "Warmup Inference :  (10859.865, 152.2026428811274)\n",
            "Avg Inference :  (10836.975, 152.91165621123568)\n",
            "Memory Init :  (42.6627, 0.05734186493029573)\n",
            "Memory Overall :  (44.53184, 0.06317522332453678)\n",
            "\n",
            "\n",
            "InceptionV3\n",
            "Init Time :  (131.5376, 1.2245915924997426)\n",
            "Init Inference :  (131537.6, 1224.5915924997414)\n",
            "First Inference :  (125093.95, 254.66499666486519)\n",
            "Warmup Inference :  (125168.5, 281.07416140678305)\n",
            "Avg Inference :  (125335.15, 138.66668669871652)\n",
            "Memory Init :  (201.9737, 0.09392836015871774)\n",
            "Memory Overall :  (204.0978, 0.09064598577345714)\n",
            "\n",
            "\n",
            "ResNet50\n",
            "Init Time :  (149.4972, 0.9566498892104223)\n",
            "Init Inference :  (149497.2, 956.6498892104236)\n",
            "First Inference :  (92202.45, 113.56402089616792)\n",
            "Warmup Inference :  (92231.425, 107.0237104816635)\n",
            "Avg Inference :  (92360.78, 99.06227493748452)\n",
            "Memory Init :  (252.9281, 0.09441950066201138)\n",
            "Memory Overall :  (254.27575, 0.09460715172699537)\n",
            "\n",
            "\n",
            "ResNet101\n",
            "Init Time :  (223.60365, 1.201862645405478)\n",
            "Init Inference :  (223603.65, 1201.8626454054775)\n",
            "First Inference :  (177134.85, 267.43150131339587)\n",
            "Warmup Inference :  (177076.2, 241.45188035365513)\n",
            "Avg Inference :  (177411.4, 213.45809400146166)\n",
            "Memory Init :  (424.6549, 0.13004772403357934)\n",
            "Memory Overall :  (425.75885, 0.0968288098710941)\n",
            "\n",
            "\n",
            "ResNet152\n",
            "Init Time :  (291.49275, 1.0900649365012427)\n",
            "Init Inference :  (291492.75, 1090.0649365012498)\n",
            "First Inference :  (258801.8, 462.85380208578084)\n",
            "Warmup Inference :  (258769.05, 384.8628566399369)\n",
            "Avg Inference :  (259667.35, 331.95279864208976)\n",
            "Memory Init :  (577.0768, 0.07902937495173479)\n",
            "Memory Overall :  (578.1666, 0.07893728356640549)\n",
            "\n",
            "\n",
            "VGG16\n",
            "Init Time :  (423.7022, 1.398719737925778)\n",
            "Init Inference :  (423702.2, 1398.7197379257782)\n",
            "First Inference :  (378240.85, 600.7281787437993)\n",
            "Warmup Inference :  (378603.85, 569.3742895310144)\n",
            "Avg Inference :  (382139.35, 417.8948841640997)\n",
            "Memory Init :  (1094.42, 0.09689929661359016)\n",
            "Memory Overall :  (1095.7675, 0.09574500124372376)\n",
            "\n",
            "\n",
            "VGG19\n",
            "Init Time :  (446.5987, 1.0500466706419824)\n",
            "Init Inference :  (446598.7, 1050.0466706419843)\n",
            "First Inference :  (475165.35, 608.0213184455052)\n",
            "Warmup Inference :  (475443.75, 432.25928804973427)\n",
            "Avg Inference :  (479833.1, 243.77770287918588)\n",
            "Memory Init :  (1135.284, 0.07969679383367774)\n",
            "Memory Overall :  (1136.374, 0.07969679383371588)\n",
            "\n",
            "\n",
            "MobileNet_quant\n",
            "Init Time :  (43.3263, 1.7335478406405382)\n",
            "Init Inference :  (43326.3, 1733.5478406405384)\n",
            "First Inference :  (9221.95, 126.17176054052165)\n",
            "Warmup Inference :  (8219.4795, 27.47107636367656)\n",
            "Avg Inference :  (8201.4765, 27.51265723549607)\n",
            "Memory Init :  (18.035363157894736, 0.06691936449959168)\n",
            "Memory Overall :  (23.207436842105263, 0.1593896734788545)\n",
            "\n",
            "\n",
            "InceptionV3_quant\n",
            "Init Time :  (76.11865, 1.333510652573243)\n",
            "Init Inference :  (76118.65, 1333.5106525732426)\n",
            "First Inference :  (29883.55, 475.1456812610766)\n",
            "Warmup Inference :  (29580.22, 472.24446122521636)\n",
            "Avg Inference :  (29571.09, 431.3770604886048)\n",
            "Memory Init :  (65.451385, 0.08269863662721375)\n",
            "Memory Overall :  (67.5754, 0.08116564609034478)\n",
            "\n",
            "\n",
            "ResNet50_quant\n",
            "Init Time :  (92.6863, 1.1209696507465037)\n",
            "Init Inference :  (92686.3, 1120.969650746503)\n",
            "First Inference :  (28545.45, 59.90121253495537)\n",
            "Warmup Inference :  (28519.675, 44.121517307020575)\n",
            "Avg Inference :  (28530.29, 43.459695786955635)\n",
            "Memory Init :  (108.10185, 0.11162638576967364)\n",
            "Memory Overall :  (109.20085, 0.1101153820699956)\n",
            "\n",
            "\n",
            "ResNet101_quant\n",
            "Init Time :  (132.43105, 1.0066134717849462)\n",
            "Init Inference :  (132431.05, 1006.6134717849485)\n",
            "First Inference :  (47169.85, 69.70182735654859)\n",
            "Warmup Inference :  (47239.375, 63.04775863122358)\n",
            "Avg Inference :  (47271.235, 59.09475241641952)\n",
            "Memory Init :  (172.19174999999998, 0.10069697899613235)\n",
            "Memory Overall :  (173.27985, 0.10269437282794967)\n",
            "\n",
            "\n",
            "ResNet152_quant\n",
            "Init Time :  (168.77125, 0.9813507809459162)\n",
            "Init Inference :  (168771.25, 981.3507809459153)\n",
            "First Inference :  (66815.25, 91.6646491006039)\n",
            "Warmup Inference :  (66847.91, 92.15588566748137)\n",
            "Avg Inference :  (66907.775, 92.2924520777632)\n",
            "Memory Init :  (236.28565, 0.08808325905821134)\n",
            "Memory Overall :  (237.37550000000002, 0.08813775103719637)\n",
            "\n",
            "\n",
            "VGG16_quant\n",
            "Init Time :  (231.5695, 1.3749214140222106)\n",
            "Init Inference :  (231569.5, 1374.9214140222132)\n",
            "First Inference :  (82801.15, 145.52853762461126)\n",
            "Warmup Inference :  (82184.66, 170.6249978600331)\n",
            "Avg Inference :  (82120.385, 171.95384409776813)\n",
            "Memory Init :  (293.82505, 0.09809365825309)\n",
            "Memory Overall :  (294.9148, 0.09813556681288274)\n",
            "\n",
            "\n",
            "VGG19_quant\n",
            "Init Time :  (237.55904999999998, 1.0444697206565727)\n",
            "Init Inference :  (237559.05, 1044.4697206565736)\n",
            "First Inference :  (102867.4, 296.60489402215654)\n",
            "Warmup Inference :  (102480.15, 266.6883209409737)\n",
            "Avg Inference :  (102407.35, 254.31171817279673)\n",
            "Memory Init :  (304.20875, 0.09086941179517238)\n",
            "Memory Overall :  (305.5563, 0.09088691643436385)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Result cmd='python3 ./experimental/measuringInferenceTimes.py' exited=0>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "node.run('python3 ./experimental/measuringInferenceTimes.py')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vFsW19_5HXm"
      },
      "source": [
        "Check if the plots for the results have been correctly saved:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbryT_Zu5HXm",
        "outputId": "63f24807-cd5b-4f6d-d28b-83b1e04a180e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avg Inference.png\n",
            "First Inference.png\n",
            "Init Inference.png\n",
            "Init Time.png\n",
            "Memory Init.png\n",
            "Memory Overall.png\n",
            "Warmup Inference.png\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Result cmd='ls ./results' exited=0>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "node.run('ls ./results')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBdrYU9s5HXn"
      },
      "source": [
        "Copy and paste the output of the following command in the Jupyter notebook terminal to transfer the plots of results to the Jupyter environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwfHvYqR5HXn",
        "outputId": "5d124684-fb46-4f9a-b7f8-602ca641f55b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "scp -ri ~/.ssh/id_rsa_chameleon cc@192.5.87.186:/home/cc/results ./work\n"
          ]
        }
      ],
      "source": [
        "print(f'scp -ri ~/.ssh/id_rsa_chameleon cc@{reserved_fip}:/home/cc/results ./work')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR4ZZaMb5HXn"
      },
      "source": [
        "The plots resulting from the experiment should not be in the /work/results directory of the Jupyter environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu-8vSAcyqAJ"
      },
      "source": [
        "## **Release Resources**\n",
        "If you finish with your experimentation before your lease expires, release your resources and tear down your environment by running the following (commented out to prevent accidental deletions).\n",
        "\n",
        "This section is designed to work as a \"standalone\" portion - you can come back to this notebook, ignore the top part, and just run this section to delete your reasources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V69kSBZWyyCl",
        "jupyter": {
          "source_hidden": true
        },
        "outputId": "5f6cd7b2-8515-40af-b6d0-6c8b9c63cfc0",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now using CHI@UC:\n",
            "URL: https://chi.uc.chameleoncloud.org\n",
            "Location: Argonne National Laboratory, Lemont, Illinois, USA\n",
            "Support contact: help@chameleoncloud.org\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# setup environment - if you made any changes in the top part, make the same changes here\n",
        "import chi, os\n",
        "from chi import lease, server\n",
        "\n",
        "PROJECT_NAME = os.getenv('OS_PROJECT_NAME')\n",
        "chi.use_site(\"CHI@UC\")\n",
        "chi.set(\"project_name\", PROJECT_NAME)\n",
        "\n",
        "\n",
        "lease = chi.lease.get_lease(f\"{username}-{NODE_TYPE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loz0O2Y9yyhz",
        "outputId": "fd2ac8e8-6e4a-4081-cbc7-c359a12a1311"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted lease with id 419acc92-7ac8-4788-972b-b706a33f0d4c\n"
          ]
        }
      ],
      "source": [
        "DELETE = True\n",
        "# DELETE = True\n",
        "\n",
        "if DELETE:\n",
        "    # delete server\n",
        "    server_id = chi.server.get_server_id(f\"{username}-{NODE_TYPE}\")\n",
        "    chi.server.delete_server(server_id)\n",
        "\n",
        "    # release floating IP\n",
        "    reserved_fip =  chi.lease.get_reserved_floating_ips(lease[\"id\"])[0]\n",
        "    ip_info = chi.network.get_floating_ip(reserved_fip)\n",
        "    chi.neutron().delete_floatingip(ip_info[\"id\"])\n",
        "\n",
        "    # delete lease\n",
        "    chi.lease.delete_lease(lease[\"id\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}