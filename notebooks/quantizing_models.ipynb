{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Quantization"
   ],
   "id": "bebb90ce-bd12-4a8f-bb05-b463495272df"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates the process of applying Quantization to Deep Learning Models."
   ],
   "id": "106c4be0-0d3e-446b-a1d4-3a522fda947e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization, a technique that compresses models by converting parameters from floating point to fixed width numbers, can be useful to prepare models for deployment on edge devices. It significantly decreases the size of models and may improve inference times as well. In this notebook, [Post-training Dynamic Range Quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) is implemented, which converts model weights from floating point to integers with 8-bit precision."
   ],
   "id": "ea810dee-2eda-4913-81a5-2853c7f7db65"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The technique is applied on popular Convolutional Neural Networks: MobileNet, InceptionV3, Resnet50, ResNet101, ResNet152, VGG16, VGG19."
   ],
   "id": "fb5cf436-25f3-4e8a-a408-2b8812b8dbed"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s install TensorFlow (version 2.17.0)."
   ],
   "id": "ae3c63a7-bd03-4288-95b9-61b0ab70142e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.17.0"
   ],
   "id": "eefc8ae1-cd2a-4f25-8f3b-e76b417e358a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s import the neccessary libraries."
   ],
   "id": "dfb41520-b843-4380-b304-60b1e519fc52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pathlib"
   ],
   "id": "62deb4f5-301c-4792-8676-43fbebe151dd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define a list of the names of the models to be quantized."
   ],
   "id": "97ab8fb6-0382-4334-b26f-d44c1fa3bfa7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNames = [\"MobileNet\", \"InceptionV3\", \"ResNet50\", \"ResNet101\", \"ResNet152\", \"VGG16\", \"VGG19\"]"
   ],
   "id": "6bb8b22a-84b7-4d10-817e-713b51228346"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models are then loaded using the Keras API. One instance of each model is converted to tflite format without any optimization; another instance is optimized using Dynamic Range Quantization."
   ],
   "id": "02900bfd-398b-4cb4-a21b-07e0b1cbe792"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the original and the quantized versions of the models are stored in the `./tflite_models directory`."
   ],
   "id": "46877238-5ed1-4a04-ae9a-1262c9258b99"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for modelName in modelNames:\n",
    "  model_class = getattr(tf.keras.applications, modelName)\n",
    "  model = model_class(weights='imagenet')\n",
    "\n",
    "  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "  tflite_model = converter.convert()\n",
    "\n",
    "  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "  tflite_model_quant = converter.convert()\n",
    "\n",
    "  tflite_models_dir = pathlib.Path(\"/root/tflite_models/\")\n",
    "  tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "  # Save the unquantized/float model:\n",
    "  tflite_model_file = tflite_models_dir/(modelName+\".tflite\")\n",
    "  tflite_model_file.write_bytes(tflite_model)\n",
    "  # Save the quantized model:\n",
    "  tflite_model_quant_file = tflite_models_dir/(modelName+\"_quant.tflite\")\n",
    "  tflite_model_quant_file.write_bytes(tflite_model_quant)"
   ],
   "id": "162642d9-1c68-4fd2-b6d7-07e4de97f49c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can verify that all models have been correctly saved by printing the contents of the `./tflite_models` directory. You should be able to see two files for each model, one for the original and one for the quantized version. Note that the size of the original models is bigger than the size of their quantized version."
   ],
   "id": "c1d92524-3275-4b07-b7ed-fd7d148c3882"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l --block-size=M ./tflite_models"
   ],
   "id": "fadbf8b2-27f8-4e5f-8791-fa8bfe53d6e8"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
