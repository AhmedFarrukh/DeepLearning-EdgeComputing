{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedFarrukh/DeepLearning-EdgeComputing/blob/main/notebooks/MeasuringInferenceTimes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrF-tC2TOA5x"
      },
      "source": [
        "In this notebook, the inference times and memory footprint of the original and quantized versions of 7 popular Convolutional Neural Networks are measured and compared.\n",
        "\n",
        "The CNN models are: MobileNet, InceptionV3, Resnet50, ResNet101, ResNet152, VGG16, VGG19.\n",
        "\n",
        "The quantized models were created by applying [Post-training Dynamic Range Quantization](https://www.tensorflow.org/lite/performance/post_training_quantization).\n",
        "\n",
        "Both the original models, and their quantized versions, are of tflite format, and were uploaded to [Google Drive](https://drive.google.com/drive/folders/1OcJ9ceYg6ZWFJ4QMR0zznsw0KVeHPa4h?usp=drive_link).\n",
        "\n",
        "The benchmarking of models is achieved by using the official [TFlite benchmark](https://www.tensorflow.org/lite/performance/measurement) which measures the following metrics:\n",
        "*   Initialization time\n",
        "*   Inference time of warmup state\n",
        "*   Inference time of steady state\n",
        "*   Memory usage during initialization time\n",
        "*   Overall memory usage\n",
        "\n",
        "The benchmark generates a series of random inputs, runs the models and aggregates the results to report the aforementioned metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToA8HeyjN7qQ"
      },
      "outputs": [],
      "source": [
        "modelNames = [\"MobileNet\", \"InceptionV3\", \"ResNet50\", \"ResNet101\", \"ResNet152\", \"VGG16\", \"VGG19\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can download the models from the Google Drive using gdown. If you want to download your own set of models, you can modify the google drive link below. In this case, we download the models to the /root/tflite_models directory."
      ],
      "metadata": {
        "id": "wZVUtaESAFA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "id": "KsKhnHT3Umfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --folder https://drive.google.com/drive/folders/1OcJ9ceYg6ZWFJ4QMR0zznsw0KVeHPa4h -O /root/tflite_models"
      ],
      "metadata": {
        "id": "jvAZKLKwAFdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can verify that the models were correctly loaded by listing the files in the /root/tflite_models directory. Note that there should be two tflite files for each model: an original and a quantized version. The size of the quantized models should be significantly smaller than the size of their corresponding original model."
      ],
      "metadata": {
        "id": "VkKMH6CMCv1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /root/tflite_models"
      ],
      "metadata": {
        "id": "-RyqChjIDEUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWcYnbyPX0SR"
      },
      "source": [
        "Next, we download the TFlite benchmark which we will use to measure inference times and memory footprint. More details about the benchmark can be found on the [tensorflow website](https://www.tensorflow.org/lite/performance/measurement). Note that the benchmark is specific to the architecture type (such as x86 or ARM), and the appropriate benchmark binary must be downloaded. Below, the benchmark is loaded for an x86-64 type architecture.\n",
        "\n",
        "The benchmark is downloaded to the /root/benchmark folder, and its permissions are then updated to allow it to be executed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1ZDxygKnKy8"
      },
      "outputs": [],
      "source": [
        "!mkdir /root/benchmark\n",
        "!wget https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/linux_x86-64_benchmark_model -P /root/benchmark\n",
        "!chmod +x /root/benchmark/linux_x86-64_benchmark_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run the benchmark on the MobileNet_quant model and note the output."
      ],
      "metadata": {
        "id": "L4yYd2m0CJ1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/root/benchmark/linux_x86-64_benchmark_model \\\n",
        "      --graph=/root/tflite_models/MobileNet_quant.tflite \\\n",
        "      --num_threads=1"
      ],
      "metadata": {
        "id": "gxwkgn7ECZv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMpKVIzvX3Ym"
      },
      "source": [
        "Since the result of the benchmark is reported as text on the console, we can define a parsing function to extract the data. The parsing function takes the output of the benchmark as an input and adds the results to a dictionary of metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dXGB-Dv0mxg"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def parse_benchmark_output(output, results):\n",
        "    \"\"\"\n",
        "    Parse benchmark output to extract model initialization times, inference timings, and memory footprint.\n",
        "    \"\"\"\n",
        "\n",
        "    # Regular expressions to match the required information\n",
        "    init_time_patterns = [\n",
        "        re.compile(r'INFO: Initialized session in (\\d+.\\d+)ms.'),\n",
        "        re.compile(r'INFO: Initialized session in (\\d+)ms.')\n",
        "    ]\n",
        "    inference_patterns = [\n",
        "        re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): (\\d+.\\d+), Inference \\(avg\\): (\\d+.\\d+)'),\n",
        "        re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): (\\d+), Inference \\(avg\\): (\\d+)'),\n",
        "        re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): ([\\d.e+]+), Inference \\(avg\\): (\\d+)')\n",
        "    ]\n",
        "    memory_pattern = re.compile(r'INFO: Memory footprint delta from the start of the tool \\(MB\\): init=(\\d+.\\d+) overall=(\\d+.\\d+)')\n",
        "\n",
        "    for line in output.split('\\n'):\n",
        "        # Match the initialization time\n",
        "        for pattern in init_time_patterns:\n",
        "            init_match = pattern.search(line)\n",
        "            if init_match:\n",
        "                results['Init Time (ms)'].append(float(init_match.group(1)))\n",
        "                break\n",
        "\n",
        "        # Match the inference timings\n",
        "        for pattern in inference_patterns:\n",
        "            inference_match = pattern.search(line)\n",
        "            if inference_match:\n",
        "                results[\"Init Inference (ms)\"].append(int(inference_match.group(1))/1000)\n",
        "                results[\"First Inference (ms)\"].append(int(inference_match.group(2))/1000)\n",
        "                results[\"Warmup Inference (ms)\"].append(float(inference_match.group(3))/1000)\n",
        "                results[\"Avg Inference (ms)\"].append(float(inference_match.group(4))/1000)\n",
        "                break\n",
        "\n",
        "        # Match the memory footprint\n",
        "        memory_match = memory_pattern.search(line)\n",
        "        if memory_match:\n",
        "            results['Memory Init (MB)'].append(float(memory_match.group(1)))\n",
        "            results['Memory Overall (MB)'].append(float(memory_match.group(2)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can define a Pandas Dataframe to store our results. Since we will be repeatedly running the benchmark to estimate the standard deviation of results as well, for each metric, we will define two columns - one for the mean and the other for the standard deviation."
      ],
      "metadata": {
        "id": "ezfpt0IGGlWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "id": "N3u_KLjkNY0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define model types (rows)\n",
        "rows = []\n",
        "for model in modelNames:\n",
        "  rows.append(model)\n",
        "  rows.append(model + \"_quant\")\n",
        "\n",
        "# Define features (columns)\n",
        "cols = []\n",
        "features = [\"Init Time (ms)\", \"Init Inference (ms)\", \"First Inference (ms)\", \"Warmup Inference (ms)\", \"Avg Inference (ms)\", \"Memory Init (MB)\", \"Memory Overall (MB)\"]\n",
        "for feature in features:\n",
        "  cols.append(feature)\n",
        "  cols.append(feature + \"_sd\")\n",
        "\n",
        "# Create an empty DataFrame\n",
        "finalResult = pd.DataFrame(index=rows, columns=cols)"
      ],
      "metadata": {
        "id": "P5DH1Ki5G2nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1BWO_62YAY4"
      },
      "source": [
        "Finally, run the benchmark repeatedly and average the results. For each model, we repeatedly run the benchmark, and parse the output from the benchmark. After `n` trials, the mean and standard deviation of the metrics is added to the `finalResult` dataframe defined in the last step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvngpXpxlpvO",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "from collections import defaultdict\n",
        "from statistics import mean\n",
        "from statistics import stdev\n",
        "\n",
        "n = 10 #the number of times the benchmark is called for each model\n",
        "\n",
        "for modelName in rows:\n",
        "  print(modelName)\n",
        "  modelResults = defaultdict(list)\n",
        "  for i in range(n):\n",
        "    outputOriginal = subprocess.check_output(\"/root/benchmark/linux_x86-64_benchmark_model \\\n",
        "      --graph=/root/tflite_models/\" + modelName +\".tflite\"+\" \\\n",
        "      --num_threads=1\", shell=True)\n",
        "    outputOriginal = outputOriginal.decode('utf-8')\n",
        "    output = parse_benchmark_output(outputOriginal, modelResults)\n",
        "\n",
        "  for feature in features:\n",
        "    finalResult.loc[modelName, feature] = mean(modelResults[feature])\n",
        "    finalResult.loc[modelName, feature + \"_sd\"] = stdev(modelResults[feature])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlGDdZ72av1e"
      },
      "source": [
        "Let's have a look at the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1y1FR0REYNCD"
      },
      "outputs": [],
      "source": [
        "print(finalResult)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can generate plots of the results."
      ],
      "metadata": {
        "id": "86e8u7c0RI6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib\n",
        "!pip install numpy"
      ],
      "metadata": {
        "id": "znWITRMqRdYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "for feature in features:\n",
        "    means_orig = finalResult.loc[modelNames, feature].values\n",
        "    errors_orig = finalResult.loc[modelNames, feature + \"_sd\"].values\n",
        "    means_quant = finalResult.loc[[model + \"_quant\" for model in modelNames], feature].values\n",
        "    errors_quant = finalResult.loc[[model + \"_quant\" for model in modelNames], feature + \"_sd\"].values\n",
        "\n",
        "\n",
        "    n_groups = len(modelNames)\n",
        "    index = np.arange(n_groups)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    bar_width = 0.35\n",
        "    opacity = 0.8\n",
        "\n",
        "    rects1 = plt.bar(index, means_orig, bar_width,\n",
        "                     alpha=opacity,\n",
        "                     yerr=errors_orig,\n",
        "                     label='Original')\n",
        "\n",
        "    rects2 = plt.bar(index + bar_width, means_quant, bar_width,\n",
        "                     alpha=opacity,\n",
        "                     yerr=errors_quant,\n",
        "                     label='Quantized')\n",
        "\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylabel(feature)\n",
        "    plt.title(f'Bar Chart for {feature}')\n",
        "    plt.xticks(index + bar_width / 2, modelNames, rotation=45)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the plot as an image\n",
        "    plt.savefig(\"/root/plots\" + feature + \"_bar_chart.png\")\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BzVfeDYXRW1c"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmWjEmg9k9L8E37YqPA4Id",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}