{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedFarrukh/DeepLearning-EdgeComputing/blob/main/notebooks/MeasuringInferenceTimes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrF-tC2TOA5x"
      },
      "source": [
        "In this notebook, 7 popular Convolutional Neural Networks are quantized, and the change in their inference times is noted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "77dSfcJtPaU6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import pathlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ToA8HeyjN7qQ"
      },
      "outputs": [],
      "source": [
        "modelNames = [\"MobileNet\", \"ResNet50\", \"ResNet101\", \"InceptionV3\", \"VGG16\", \"VGG19\", \"ResNet152\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwkPgqOwPbhU"
      },
      "outputs": [],
      "source": [
        "for modelName in modelNames:\n",
        "  model_class = getattr(tf.keras.applications, modelName)\n",
        "  model = model_class(weights='imagenet')\n",
        "\n",
        "  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "  tflite_model = converter.convert()\n",
        "\n",
        "  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  tflite_model_quant = converter.convert()\n",
        "\n",
        "  tflite_models_dir = pathlib.Path(\"/tmp/tflite_models/\")\n",
        "  tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "  # Save the unquantized/float model:\n",
        "  tflite_model_file = tflite_models_dir/(modelName+\".tflite\")\n",
        "  tflite_model_file.write_bytes(tflite_model)\n",
        "  # Save the quantized model:\n",
        "  tflite_model_quant_file = tflite_models_dir/(modelName+\"_quant.tflite\")\n",
        "  tflite_model_quant_file.write_bytes(tflite_model_quant)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, load the benchmark."
      ],
      "metadata": {
        "id": "kWcYnbyPX0SR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /tmp/benchmark\n",
        "!wget https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/linux_x86-64_benchmark_model -P /tmp/benchmark\n",
        "!chmod +x /tmp/benchmark/linux_x86-64_benchmark_model\n",
        "!touch /tmp/benchmark/results"
      ],
      "metadata": {
        "id": "b1ZDxygKnKy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, define a parsing function to parse the output of the benchmark."
      ],
      "metadata": {
        "id": "XMpKVIzvX3Ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def parse_benchmark_output(output):\n",
        "    results = {}\n",
        "\n",
        "    # Regular expressions to match the required information\n",
        "    model_name_pattern = re.compile(r'INFO: Graph: \\[(.*)\\]')\n",
        "    init_time_pattern = re.compile(r'INFO: Initialized session in (\\d+.\\d+)ms.')\n",
        "    inference_pattern = re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): (\\d+.\\d+), Inference \\(avg\\): (\\d+.\\d+)')\n",
        "    inference_pattern2 = re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): (\\d+), Inference \\(avg\\): (\\d+)')\n",
        "    memory_pattern = re.compile(r'INFO: Memory footprint delta from the start of the tool \\(MB\\): init=(\\d+.\\d+) overall=(\\d+.\\d+)')\n",
        "\n",
        "    # Initialize current model name\n",
        "    current_model = None\n",
        "\n",
        "    # Split the output into lines and iterate through each line\n",
        "    for line in output.split('\\n'):\n",
        "        # Check for the model name\n",
        "        model_match = model_name_pattern.search(line)\n",
        "        if model_match:\n",
        "            current_model = model_match.group(1).split('/')[-1].split('.')[0]  # Extract model name from the path\n",
        "            results[current_model] = {}\n",
        "            continue\n",
        "\n",
        "        # Check for the initialization time\n",
        "        init_match = init_time_pattern.search(line)\n",
        "        if init_match and current_model:\n",
        "            results[current_model]['Init Time (ms)'] = float(init_match.group(1))\n",
        "            continue\n",
        "\n",
        "        # Check for the inference timings\n",
        "        inference_match = inference_pattern.search(line)\n",
        "        if not inference_match:\n",
        "            inferenece_match = inference_pattern2.search(line)\n",
        "        if inference_match and current_model:\n",
        "            results[current_model]['Inference Timings (us)'] = {\n",
        "                'Init': int(inference_match.group(1)),\n",
        "                'First Inference': int(inference_match.group(2)),\n",
        "                'Warmup (avg)': float(inference_match.group(3)),\n",
        "                'Inference (avg)': float(inference_match.group(4))\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        # Check for the memory footprint\n",
        "        memory_match = memory_pattern.search(line)\n",
        "        if memory_match and current_model:\n",
        "            results[current_model]['Memory Footprint (MB)'] = {\n",
        "                'Init': float(memory_match.group(1)),\n",
        "                'Overall': float(memory_match.group(2))\n",
        "            }\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "2dXGB-Dv0mxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, run the benchmark repeatedly and average the results."
      ],
      "metadata": {
        "id": "S1BWO_62YAY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "from collections import defaultdict\n",
        "from statistics import mean\n",
        "from statistics import stdev\n",
        "\n",
        "results = defaultdict(list)\n",
        "\n",
        "numModels = len(modelNames)\n",
        "allModels = []\n",
        "for i in range(numModels):\n",
        "  allModels.append(modelNames[i])\n",
        "  allModels.append(modelNames[i] + \"_quant\")\n",
        "\n",
        "n = 5\n",
        "\n",
        "for modelName in allModels:\n",
        "  init_time = []\n",
        "  init_inference = []\n",
        "  first_inference = []\n",
        "  warmup_inference = []\n",
        "  inference = []\n",
        "  memory_init = []\n",
        "  memory_overall = []\n",
        "  for i in range(n):\n",
        "    output = subprocess.check_output(\"/tmp/benchmark/linux_x86-64_benchmark_model \\\n",
        "      --graph=/tmp/tflite_models/\" + modelName +\".tflite\"+\" \\\n",
        "      --num_threads=1\", shell=True)\n",
        "    output = output.decode('utf-8')\n",
        "    output = parse_benchmark_output(output)\n",
        "    init_time.append(output[modelName]['Init Time (ms)'])\n",
        "    init_inference.append(output[modelName]['Inference Timings (us)']['Init'])\n",
        "    first_inference.append(output[modelName]['Inference Timings (us)']['First Inference'])\n",
        "    warmup_inference.append(output[modelName]['Inference Timings (us)']['Warmup (avg)'])\n",
        "    inference.append(output[modelName]['Inference Timings (us)']['Inference (avg)'])\n",
        "    memory_init.append(output[modelName]['Memory Footprint (MB)']['Init'])\n",
        "    memory_overall.append(output[modelName]['Memory Footprint (MB)']['Overall'])\n",
        "\n",
        "  results[\"Init Time\"].append((mean(init_time), stdev(init_time)))\n",
        "  results[\"Init Inference\"].append((mean(init_inference), stdev(init_inference)))\n",
        "  results[\"First Inference\"].append((mean(first_inference), stdev(first_inference)))\n",
        "  results[\"Warmup Inference\"].append((mean(warmup_inference), stdev(warmup_inference)))\n",
        "  results[\"Avg Inference\"].append((mean(inference), stdev(inference)))\n",
        "  results[\"Memory Init\"].append((mean(memory_init), stdev(memory_init)))\n",
        "  results[\"Memory Overall\"].append((mean(memory_overall), stdev(memory_overall)))"
      ],
      "metadata": {
        "id": "kvngpXpxlpvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(allModels)):\n",
        "  print(allModels[i])\n",
        "  for key in results:\n",
        "    print(key, \": \", results[key][i])"
      ],
      "metadata": {
        "id": "1y1FR0REYNCD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxpGXUCH9RrEu6e3XT84Uz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}