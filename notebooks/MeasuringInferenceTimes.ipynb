{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedFarrukh/DeepLearning-EdgeComputing/blob/main/notebooks/MeasuringInferenceTimes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrF-tC2TOA5x"
      },
      "source": [
        "In this notebook, the inference times and memory footprint of the original and quantized versions of 7 popular Convolutional Neural Networks are measured and compared.\n",
        "\n",
        "The CNN models are: MobileNet, InceptionV3, Resnet50, ResNet101, ResNet152, VGG16, VGG19.\n",
        "\n",
        "The quantized models were created by applying [Post-training Dynamic Range Quantization](https://www.tensorflow.org/lite/performance/post_training_quantization).\n",
        "\n",
        "Both the original models, and their quantized versions, are of tflite format, and were uploaded to [Google Drive](https://drive.google.com/drive/folders/1OcJ9ceYg6ZWFJ4QMR0zznsw0KVeHPa4h?usp=drive_link)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToA8HeyjN7qQ"
      },
      "outputs": [],
      "source": [
        "modelNames = [\"MobileNet\", \"ResNet50\", \"ResNet101\", \"InceptionV3\", \"VGG16\", \"VGG19\", \"ResNet152\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77dSfcJtPaU6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import pathlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWcYnbyPX0SR"
      },
      "source": [
        "Next, load the benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1ZDxygKnKy8"
      },
      "outputs": [],
      "source": [
        "!mkdir /tmp/benchmark\n",
        "!wget https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/linux_x86-64_benchmark_model -P /tmp/benchmark\n",
        "!chmod +x /tmp/benchmark/linux_x86-64_benchmark_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMpKVIzvX3Ym"
      },
      "source": [
        "Next, define a parsing function to parse the output of the benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dXGB-Dv0mxg"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def parse_benchmark_output(output):\n",
        "    \"\"\"\n",
        "    Parse benchmark output to extract model initialization times, inference timings, and memory footprint.\n",
        "\n",
        "    :param output: The raw output string from the benchmark.\n",
        "    :return: A dictionary containing parsed benchmark results.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # Regular expressions to match the required information\n",
        "    model_name_pattern = re.compile(r'INFO: Graph: \\[(.*)\\]')\n",
        "    init_time_pattern = re.compile(r'INFO: Initialized session in (\\d+.\\d+)ms.')\n",
        "    inference_patterns = [\n",
        "        re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): (\\d+.\\d+), Inference \\(avg\\): (\\d+.\\d+)'),\n",
        "        re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): (\\d+), Inference \\(avg\\): (\\d+)'),\n",
        "        re.compile(r'INFO: Inference timings in us: Init: (\\d+), First inference: (\\d+), Warmup \\(avg\\): ([\\d.e+]+), Inference \\(avg\\): (\\d+)')\n",
        "    ]\n",
        "    memory_pattern = re.compile(r'INFO: Memory footprint delta from the start of the tool \\(MB\\): init=(\\d+.\\d+) overall=(\\d+.\\d+)')\n",
        "\n",
        "    current_model = None\n",
        "\n",
        "    for line in output.split('\\n'):\n",
        "        # Match the model name\n",
        "        model_match = model_name_pattern.search(line)\n",
        "        if model_match:\n",
        "            current_model = model_match.group(1).split('/')[-1].split('.')[0]\n",
        "            results[current_model] = {}\n",
        "            continue\n",
        "\n",
        "        # Match the initialization time\n",
        "        init_match = init_time_pattern.search(line)\n",
        "        if init_match and current_model:\n",
        "            results[current_model]['Init Time (ms)'] = float(init_match.group(1))\n",
        "            continue\n",
        "\n",
        "        # Match the inference timings\n",
        "        for pattern in inference_patterns:\n",
        "            inference_match = pattern.search(line)\n",
        "            if inference_match and current_model:\n",
        "                results[current_model]['Inference Timings (us)'] = {\n",
        "                    'Init': int(inference_match.group(1)),\n",
        "                    'First Inference': int(inference_match.group(2)),\n",
        "                    'Warmup (avg)': float(inference_match.group(3)),\n",
        "                    'Inference (avg)': float(inference_match.group(4))\n",
        "                }\n",
        "                break\n",
        "\n",
        "        # Match the memory footprint\n",
        "        memory_match = memory_pattern.search(line)\n",
        "        if memory_match and current_model:\n",
        "            results[current_model]['Memory Footprint (MB)'] = {\n",
        "                'Init': float(memory_match.group(1)),\n",
        "                'Overall': float(memory_match.group(2))\n",
        "            }\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1BWO_62YAY4"
      },
      "source": [
        "Finally, run the benchmark repeatedly and average the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvngpXpxlpvO"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "from collections import defaultdict\n",
        "from statistics import mean\n",
        "from statistics import stdev\n",
        "\n",
        "results = defaultdict(list)\n",
        "\n",
        "numModels = len(modelNames)\n",
        "allModels = modelNames.copy()\n",
        "for i in range(numModels):\n",
        "  allModels.append(modelNames[i] + \"_quant\")\n",
        "\n",
        "n = 20\n",
        "\n",
        "for modelName in allModels:\n",
        "  print(modelName)\n",
        "  init_time = []\n",
        "  init_inference = []\n",
        "  first_inference = []\n",
        "  warmup_inference = []\n",
        "  inference = []\n",
        "  memory_init = []\n",
        "  memory_overall = []\n",
        "  for i in range(n):\n",
        "    outputOriginal = subprocess.check_output(\"/tmp/benchmark/linux_x86-64_benchmark_model \\\n",
        "      --graph=/tmp/tflite_models/\" + modelName +\".tflite\"+\" \\\n",
        "      --num_threads=1\", shell=True)\n",
        "    outputOriginal = outputOriginal.decode('utf-8')\n",
        "    output = parse_benchmark_output(outputOriginal)\n",
        "    try:\n",
        "      init_time.append(output[modelName]['Init Time (ms)'])\n",
        "      init_inference.append(output[modelName]['Inference Timings (us)']['Init'])\n",
        "      first_inference.append(output[modelName]['Inference Timings (us)']['First Inference'])\n",
        "      warmup_inference.append(output[modelName]['Inference Timings (us)']['Warmup (avg)'])\n",
        "      inference.append(output[modelName]['Inference Timings (us)']['Inference (avg)'])\n",
        "      memory_init.append(output[modelName]['Memory Footprint (MB)']['Init'])\n",
        "      memory_overall.append(output[modelName]['Memory Footprint (MB)']['Overall'])\n",
        "    except: #error in parsing\n",
        "      print(\"Error with model: \", modelName)\n",
        "      print(output)\n",
        "      print(outputOriginal)\n",
        "      continue\n",
        "\n",
        "  results[\"Init Time\"].append((mean(init_time), stdev(init_time)))\n",
        "  results[\"Init Inference\"].append((mean(init_inference), stdev(init_inference)))\n",
        "  results[\"First Inference\"].append((mean(first_inference), stdev(first_inference)))\n",
        "  results[\"Warmup Inference\"].append((mean(warmup_inference), stdev(warmup_inference)))\n",
        "  results[\"Avg Inference\"].append((mean(inference), stdev(inference)))\n",
        "  results[\"Memory Init\"].append((mean(memory_init), stdev(memory_init)))\n",
        "  results[\"Memory Overall\"].append((mean(memory_overall), stdev(memory_overall)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlGDdZ72av1e"
      },
      "source": [
        "The following code prints the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1y1FR0REYNCD"
      },
      "outputs": [],
      "source": [
        "for i in range(len(allModels)):\n",
        "  print(allModels[i])\n",
        "  for key in results:\n",
        "    print(key, \": \", results[key][i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLp33EcOyMrL"
      },
      "outputs": [],
      "source": [
        "!mkdir /tmp/plots\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "for key in results:\n",
        "    means = [x[0] for x in results[key]]\n",
        "    errors = [x[1] for x in results[key]]\n",
        "\n",
        "    n_groups = len(modelNames)\n",
        "    index = np.arange(n_groups)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    bar_width = 0.35\n",
        "    opacity = 0.8\n",
        "\n",
        "    rects1 = plt.bar(index, means[:n_groups], bar_width,\n",
        "                     alpha=opacity,\n",
        "                     yerr=errors[:n_groups],\n",
        "                     label=f'{key} (Original)')\n",
        "\n",
        "    rects2 = plt.bar(index + bar_width, means[n_groups:], bar_width,\n",
        "                     alpha=opacity,\n",
        "                     yerr=errors[n_groups:],\n",
        "                     label=f'{key} (Quantized)')\n",
        "\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylabel('Values')\n",
        "    plt.title(f'Bar Chart for {key}')\n",
        "    plt.xticks(index + bar_width / 2, modelNames, rotation=45)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the plot as an image\n",
        "    plt.savefig(\"/tmp/plots\" + key + \"_bar_chart.png\")\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcsJOsGTbjTBQRzCgmkcSy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}